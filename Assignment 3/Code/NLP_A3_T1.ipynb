{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/lijiayi/opt/anaconda3/lib/python3.9/site-packages (1.11.0)\n",
      "Requirement already satisfied: torchvision in /Users/lijiayi/opt/anaconda3/lib/python3.9/site-packages (0.12.0)\n",
      "Requirement already satisfied: transformers in /Users/lijiayi/opt/anaconda3/lib/python3.9/site-packages (4.18.0)\n",
      "Requirement already satisfied: tdqm in /Users/lijiayi/opt/anaconda3/lib/python3.9/site-packages (0.0.1)\n",
      "Collecting clean-text\n",
      "  Downloading clean_text-0.6.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: typing-extensions in /Users/lijiayi/opt/anaconda3/lib/python3.9/site-packages (from torch) (3.10.0.2)\n",
      "Requirement already satisfied: requests in /Users/lijiayi/opt/anaconda3/lib/python3.9/site-packages (from torchvision) (2.26.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/lijiayi/opt/anaconda3/lib/python3.9/site-packages (from torchvision) (8.4.0)\n",
      "Requirement already satisfied: numpy in /Users/lijiayi/opt/anaconda3/lib/python3.9/site-packages (from torchvision) (1.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/lijiayi/opt/anaconda3/lib/python3.9/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/lijiayi/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/lijiayi/opt/anaconda3/lib/python3.9/site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /Users/lijiayi/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/lijiayi/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: filelock in /Users/lijiayi/opt/anaconda3/lib/python3.9/site-packages (from transformers) (3.3.1)\n",
      "Requirement already satisfied: sacremoses in /Users/lijiayi/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.0.53)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/lijiayi/opt/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Collecting emoji<2.0.0,>=1.0.0\n",
      "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
      "\u001b[K     |████████████████████████████████| 175 kB 9.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ftfy<7.0,>=6.0\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "\u001b[K     |████████████████████████████████| 53 kB 3.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /Users/lijiayi/opt/anaconda3/lib/python3.9/site-packages (from ftfy<7.0,>=6.0->clean-text) (0.2.5)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/lijiayi/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/lijiayi/opt/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lijiayi/opt/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/lijiayi/opt/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/lijiayi/opt/anaconda3/lib/python3.9/site-packages (from requests->torchvision) (3.2)\n",
      "Requirement already satisfied: click in /Users/lijiayi/opt/anaconda3/lib/python3.9/site-packages (from sacremoses->transformers) (8.0.3)\n",
      "Requirement already satisfied: joblib in /Users/lijiayi/opt/anaconda3/lib/python3.9/site-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: six in /Users/lijiayi/opt/anaconda3/lib/python3.9/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Building wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171047 sha256=7dc07996a0a6c34a64775aa2ac92c68cbdbfc9257202409211828749576b83ac\n",
      "  Stored in directory: /Users/lijiayi/Library/Caches/pip/wheels/fa/7a/e9/22dd0515e1bad255e51663ee513a2fa839c95934c5fc301090\n",
      "Successfully built emoji\n",
      "Installing collected packages: ftfy, emoji, clean-text\n",
      "Successfully installed clean-text-0.6.0 emoji-1.7.0 ftfy-6.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision transformers tdqm clean-text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data from train-data, dev-data, test-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/Users/lijiayi/Desktop/project-data/'\n",
    "train_ids_path = '/Users/lijiayi/Desktop/project-data/train.data.txt'\n",
    "train_label_path = '/Users/lijiayi/Desktop/project-data/train.label.txt'\n",
    "dev_ids_path = '/Users/lijiayi/Desktop/project-data/dev.data.txt'\n",
    "dev_label_path = '/Users/lijiayi/Desktop/project-data/dev.label.txt'\n",
    "test_ids_path = '/Users/lijiayi/Desktop/project-data/test.data.txt'\n",
    "\n",
    "train_ids = open(train_ids_path, 'r').readlines()\n",
    "train_labels = open(train_label_path, 'r').readlines()\n",
    "dev_ids = open(dev_ids_path, 'r').readlines()\n",
    "dev_labels = open(dev_label_path, 'r').readlines()\n",
    "test_ids = open(test_ids_path, 'r').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_preparation(train_ids, train_labels):\n",
    "    tweets = []\n",
    "    texts = []\n",
    "    int_labels = []\n",
    "    for ids, label in zip(train_ids, train_labels):\n",
    "        ## finding the tweet object and text with corresponding id\n",
    "        id_list = ids.strip().split(\",\")\n",
    "        tweet_objects = []\n",
    "        tweet_texts = []\n",
    "        ## check if the source tweet has been crawled ans stored in os, otherwise skip\n",
    "        if os.path.exists(root_dir + 'train-data/' + id_list[0] + '.json'):\n",
    "            for tid in id_list:\n",
    "                data_file_path = root_dir + 'train-data/' + tid + '.json'\n",
    "                ## check if each tweet has crawled data, otherwise skip \n",
    "                if os.path.exists(data_file_path):\n",
    "                    tweet = open(data_file_path)\n",
    "                    data = json.load(tweet)\n",
    "                    tweet_objects.append(data)\n",
    "                    tweet.close\n",
    "            ## sort the tweet objects by timestamp\n",
    "            tweet_objects = sorted(tweet_objects, key=lambda x: datetime.timestamp(datetime.strptime(x['created_at'], '%Y-%m-%dT%H:%M:%S.%fZ')))\n",
    "            tweets.append(tweet_objects)\n",
    "\n",
    "            ## put texts together as a context\n",
    "            for obj in tweet_objects:\n",
    "                tweet_texts.append(obj['text'])\n",
    "            texts.append(tweet_texts)\n",
    "\n",
    "            ## convert string label to int label\n",
    "            int_label = 0 if label.strip() == 'nonrumour' else 1\n",
    "            int_labels.append(int_label)\n",
    "\n",
    "    return tweets, texts, int_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev_data_preparation(dev_ids, dev_labels):\n",
    "    tweets = []\n",
    "    texts = []\n",
    "    int_labels = []\n",
    "    for ids, label in zip(dev_ids, dev_labels):\n",
    "        ## finding the tweet object and text with corresponding id\n",
    "        id_list = ids.strip().split(\",\")\n",
    "        tweet_objects = []\n",
    "        tweet_texts = []\n",
    "        ## check if the source tweet has been crawled ans stored in os, otherwise skip\n",
    "        if os.path.exists(root_dir + 'dev-data/' + id_list[0] + '.json'):\n",
    "            for tid in id_list:\n",
    "                data_file_path = root_dir + 'dev-data/' + tid + '.json'\n",
    "                ## check if each tweet has crawled data, otherwise skip \n",
    "                if os.path.exists(data_file_path):\n",
    "                    tweet = open(data_file_path)\n",
    "                    data = json.load(tweet)\n",
    "                    tweet_objects.append(data)\n",
    "                    tweet.close\n",
    "            ## sort the tweet objects by timestamp\n",
    "            tweet_objects = sorted(tweet_objects, key=lambda x: datetime.timestamp(datetime.strptime(x['created_at'], '%Y-%m-%dT%H:%M:%S.%fZ')))\n",
    "            tweets.append(tweet_objects)\n",
    "\n",
    "            ## put texts together as a context\n",
    "            for obj in tweet_objects:\n",
    "                tweet_texts.append(obj['text'])\n",
    "            texts.append(tweet_texts)\n",
    "\n",
    "            ## convert string label to int label\n",
    "            int_label = 0 if label.strip() == 'nonrumour' else 1\n",
    "            int_labels.append(int_label)\n",
    "\n",
    "    return tweets, texts, int_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data_preparation(test_ids):\n",
    "    tweets = []\n",
    "    texts = []\n",
    "    for ids in test_ids:\n",
    "        ## finding the tweet object and text with corresponding id\n",
    "        id_list = ids.strip().split(\",\")\n",
    "        tweet_objects = []\n",
    "        tweet_texts = []\n",
    "        ## check if the source tweet has been crawled ans stored in os, otherwise skip\n",
    "        if os.path.exists(root_dir + 'tweet-objects/' + id_list[0] + '.json'):\n",
    "            for tid in id_list:\n",
    "                data_file_path = root_dir + 'tweet-objects/' + tid + '.json'\n",
    "                ## check if each tweet has crawled data, otherwise skip \n",
    "                if os.path.exists(data_file_path):\n",
    "                    tweet = open(data_file_path)\n",
    "                    data = json.load(tweet)\n",
    "                    tweet_objects.append(data)\n",
    "                    tweet.close\n",
    "            ## sort the tweet objects by timestamp\n",
    "            tweet_objects = sorted(tweet_objects, key=lambda x: datetime.timestamp(datetime.strptime(x['created_at'], '%a %b %d %H:%M:%S +0000 %Y')))\n",
    "            tweets.append(tweet_objects)\n",
    "\n",
    "            ## put texts together as a context\n",
    "            for obj in tweet_objects:\n",
    "                tweet_texts.append(obj['text'])\n",
    "            texts.append(tweet_texts)\n",
    "\n",
    "    return tweets, texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets, train_texts, train_int_labels = train_data_preparation(train_ids, train_labels)\n",
    "dev_tweets, dev_texts, dev_int_labels = dev_data_preparation(dev_ids, dev_labels)\n",
    "test_tweets, test_texts = test_data_preparation(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1560 1560\n",
      "535 535\n",
      "558\n"
     ]
    }
   ],
   "source": [
    "print(len(train_texts), len(train_int_labels))\n",
    "print(len(dev_texts), len(dev_int_labels))\n",
    "print(len(test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4. Can eating garlic help prevent infection with the new coronavirus? #COVID19Malaysia https://t.co/q133xXBiwl',\n",
       " '5. Can regularly rinsing your nose with saline help prevent infection with the new coronavirus? https://t.co/ccMjhhD7BK',\n",
       " '6. Do vaccines against pneumonia protect you against the new coronavirus? https://t.co/wL0mlEqU95',\n",
       " '7. Can spraying alcohol or chlorine all over your body kill the new coronavirus? #Chamber https://t.co/zunVR7Ht0V',\n",
       " '8. How effective are thermal scanners in detecting people infected with the new coronavirus? https://t.co/nyLOyKAb1H',\n",
       " '9. Can an ultraviolet disinfection lamp kill the new coronavirus? https://t.co/ZrlllbkIjm',\n",
       " '10. Are hand dryers effective in killing the new coronavirus? https://t.co/cSDKXO1bGr',\n",
       " '11. The new coronavirus CANNOT be transmitted through mosquito bites. https://t.co/ZRL8bjRkpl',\n",
       " '12. Taking a hot bath does not prevent the new coronavirus disease https://t.co/bICOqSTOuD',\n",
       " '13. Cold weather and snow CANNOT kill the new coronavirus. https://t.co/7yeQQ6gLNo',\n",
       " '14. COVID-19 virus can be transmitted in areas with hot and humid climates https://t.co/ylKa2F40vu',\n",
       " '15. Drinking alcohol does not protect you against COVID-19 and can be dangerous https://t.co/ZrLN61q046',\n",
       " '16. Being able to hold your breath for 10 seconds or more without coughing or feeling discomfort DOES NOT mean you are free from the coronavirus disease (COVID-19) or any other lung disease. https://t.co/gPUL51Y2lx',\n",
       " '17. You can recover from the coronavirus disease (COVID-19). Catching the new coronavirus DOES NOT mean you will have it for life. https://t.co/yrjUM5qniK',\n",
       " '18. Exposing yourself to the sun or to temperatures higher than 25C degrees DOES NOT prevent the coronavirus disease (COVID-19) https://t.co/aOQKrrwaBv',\n",
       " '19. 5G mobile networks DO NOT spread COVID-19 https://t.co/VjqelBmpTn']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocress for train-data, dev-data, test-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleantext import clean\n",
    "\n",
    "def text_preprocess(tweet_text):\n",
    "    new_text = []\n",
    "    for t in tweet_text.split(\" \"):\n",
    "        if t in set(stopwords.words('english')):\n",
    "            continue\n",
    "        t = clean(t, no_emoji=True)\n",
    "        t = t.lstrip('@') if t.startswith('@') and len(t) > 1 else t\n",
    "        t = t.lstrip('#') if t.startswith('#') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    #new_text.append('[SEP] ')\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(_texts, _labels):\n",
    "    total_list = []\n",
    "    for event, label in zip(_texts, _labels):\n",
    "        temp_dict = {}\n",
    "        source = event[0]\n",
    "        reply_list = event[1:]\n",
    "        \n",
    "        text_concat = []\n",
    "        for reply in reply_list:   \n",
    "            text_concat.append(text_preprocess(reply))\n",
    "        reply_concat = \" \".join(text_concat)\n",
    "        \n",
    "        temp_dict['source']= text_preprocess(source)\n",
    "        temp_dict['reply'] = reply_concat\n",
    "        \n",
    "        #temp_dict['text'] = text_concat\n",
    "        temp_dict['label'] = label\n",
    "        event_ids = _texts.index(event)\n",
    "        temp_dict['event_id']  = event_ids\n",
    "        total_list.append(temp_dict)\n",
    "        \n",
    "    total_df = pd.DataFrame(total_list)\n",
    "    return total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_dataframe(_texts):\n",
    "    total_list = []\n",
    "    for event in _texts:\n",
    "        temp_dict = {}\n",
    "        source = event[0]\n",
    "        reply_list = event[1:]\n",
    "        \n",
    "        text_concat = []\n",
    "        for reply in reply_list:   \n",
    "            text_concat.append(text_preprocess(reply))\n",
    "        reply_concat = \" \".join(text_concat)\n",
    "        \n",
    "        temp_dict['source']= text_preprocess(source)\n",
    "        temp_dict['reply'] = reply_concat\n",
    "            \n",
    "        #temp_dict['text'] = text_concat\n",
    "        event_ids = _texts.index(event)\n",
    "        temp_dict['event_id']  = event_ids\n",
    "        total_list.append(temp_dict)\n",
    "        \n",
    "    total_df = pd.DataFrame(total_list)\n",
    "    return total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = create_dataframe(train_texts, train_int_labels)\n",
    "dev_df = create_dataframe(dev_texts, dev_int_labels)\n",
    "test_df = create_test_dataframe(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>reply</th>\n",
       "      <th>label</th>\n",
       "      <th>event_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4. can eating garlic help prevent infection ne...</td>\n",
       "      <td>5. can regularly rinsing nose saline help prev...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>french police chief killed charliehebdo attack...</td>\n",
       "      <td>telegraph how sad. telegraph telegraphnews the...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>coronavirus disease (covid-19) advice public: ...</td>\n",
       "      <td>infection control suspected confirmed covid-19...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ottawa police confirm multiple suspects shooti...</td>\n",
       "      <td>wsj killers go berserk cornered.  henceforth, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>primary focus government alleviate suffering/d...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1555</th>\n",
       "      <td>4. it cannot transmitted goods manufactured ch...</td>\n",
       "      <td>corona\\nall graphics taken world health organi...</td>\n",
       "      <td>0</td>\n",
       "      <td>1555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1556</th>\n",
       "      <td>desperate ted cruz claims planned parenthood s...</td>\n",
       "      <td>bipartisanism desperate! bipartisanism  cruz o...</td>\n",
       "      <td>1</td>\n",
       "      <td>1556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1557</th>\n",
       "      <td>\"thoughts prayers enough.\" pres. obama speaks ...</td>\n",
       "      <td>.@abc anyone else noticed mass shootings skyro...</td>\n",
       "      <td>1</td>\n",
       "      <td>1557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1558</th>\n",
       "      <td>police surrounded building suspected charliehe...</td>\n",
       "      <td>nbcnews bury hole nbcnews wikileakstruck cops ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1559</th>\n",
       "      <td>excellent. http</td>\n",
       "      <td>rosierawle tasha i little giggle this. hope we...</td>\n",
       "      <td>0</td>\n",
       "      <td>1559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1560 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 source  \\\n",
       "0     4. can eating garlic help prevent infection ne...   \n",
       "1     french police chief killed charliehebdo attack...   \n",
       "2     coronavirus disease (covid-19) advice public: ...   \n",
       "3     ottawa police confirm multiple suspects shooti...   \n",
       "4     primary focus government alleviate suffering/d...   \n",
       "...                                                 ...   \n",
       "1555  4. it cannot transmitted goods manufactured ch...   \n",
       "1556  desperate ted cruz claims planned parenthood s...   \n",
       "1557  \"thoughts prayers enough.\" pres. obama speaks ...   \n",
       "1558  police surrounded building suspected charliehe...   \n",
       "1559                                    excellent. http   \n",
       "\n",
       "                                                  reply  label  event_id  \n",
       "0     5. can regularly rinsing nose saline help prev...      0         0  \n",
       "1     telegraph how sad. telegraph telegraphnews the...      1         1  \n",
       "2     infection control suspected confirmed covid-19...      0         2  \n",
       "3     wsj killers go berserk cornered.  henceforth, ...      0         3  \n",
       "4                                                            0         4  \n",
       "...                                                 ...    ...       ...  \n",
       "1555  corona\\nall graphics taken world health organi...      0      1555  \n",
       "1556  bipartisanism desperate! bipartisanism  cruz o...      1      1556  \n",
       "1557  .@abc anyone else noticed mass shootings skyro...      1      1557  \n",
       "1558  nbcnews bury hole nbcnews wikileakstruck cops ...      0      1558  \n",
       "1559  rosierawle tasha i little giggle this. hope we...      0      1559  \n",
       "\n",
       "[1560 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>reply</th>\n",
       "      <th>label</th>\n",
       "      <th>event_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>covid-19 fact:\\nare hand dryers effective kill...</td>\n",
       "      <td>weatherbug they are, fact, germ-breeding facto...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>atruchecks expect result husband's \" pending\"a...</td>\n",
       "      <td>ewart_lynne atruchecks hi luck? in boat? ot_ch...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i've read lot corona virus lately i think i sh...</td>\n",
       "      <td>what coronavirus? coronavirus large family vir...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>every news outlet using headlines like,\\n\"are ...</td>\n",
       "      <td>tuckyaalto apparently, headline question, answ...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>researcher naskrecki encounter goliath birdeat...</td>\n",
       "      <td>harvard naskrecki eu tenho uma dessas em casa ...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>what are the treatment options for covid-19 (i...</td>\n",
       "      <td>antibiotics work. antibiotics used means preve...</td>\n",
       "      <td>0</td>\n",
       "      <td>454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>after speculation he's arrested, banksy debuts...</td>\n",
       "      <td>artnet xklamation story saying someone vandali...</td>\n",
       "      <td>1</td>\n",
       "      <td>531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>*your questions answered*\\n*reply number get c...</td>\n",
       "      <td>s?\\n14. can i catch covid-19 infected surfaces...</td>\n",
       "      <td>0</td>\n",
       "      <td>532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>&gt;#anonymous operation kkk &gt;ku klux klan, we ne...</td>\n",
       "      <td>anonymousvideo http grannyrosie3 anonymousvide...</td>\n",
       "      <td>1</td>\n",
       "      <td>533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>just fast virus spread misinformation surround...</td>\n",
       "      <td>5g mobile networks do not spread covid-19\\nvir...</td>\n",
       "      <td>0</td>\n",
       "      <td>534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>535 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                source  \\\n",
       "0    covid-19 fact:\\nare hand dryers effective kill...   \n",
       "1    atruchecks expect result husband's \" pending\"a...   \n",
       "2    i've read lot corona virus lately i think i sh...   \n",
       "3    every news outlet using headlines like,\\n\"are ...   \n",
       "4    researcher naskrecki encounter goliath birdeat...   \n",
       "..                                                 ...   \n",
       "530  what are the treatment options for covid-19 (i...   \n",
       "531  after speculation he's arrested, banksy debuts...   \n",
       "532  *your questions answered*\\n*reply number get c...   \n",
       "533  >#anonymous operation kkk >ku klux klan, we ne...   \n",
       "534  just fast virus spread misinformation surround...   \n",
       "\n",
       "                                                 reply  label  event_id  \n",
       "0    weatherbug they are, fact, germ-breeding facto...      0         0  \n",
       "1    ewart_lynne atruchecks hi luck? in boat? ot_ch...      0         1  \n",
       "2    what coronavirus? coronavirus large family vir...      0         2  \n",
       "3    tuckyaalto apparently, headline question, answ...      0         3  \n",
       "4    harvard naskrecki eu tenho uma dessas em casa ...      0         4  \n",
       "..                                                 ...    ...       ...  \n",
       "530  antibiotics work. antibiotics used means preve...      0       454  \n",
       "531  artnet xklamation story saying someone vandali...      1       531  \n",
       "532  s?\\n14. can i catch covid-19 infected surfaces...      0       532  \n",
       "533  anonymousvideo http grannyrosie3 anonymousvide...      1       533  \n",
       "534  5g mobile networks do not spread covid-19\\nvir...      0       534  \n",
       "\n",
       "[535 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>reply</th>\n",
       "      <th>event_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>how does covid-19 spread? http http</td>\n",
       "      <td>wcco thanks, wcco! you station i trust media p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>brain_warrior if don't believe me, look change...</td>\n",
       "      <td>brain_warrior tell people \"protesting\". brain_...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>q. how covid-19 influenza viruses similar?\\n#c...</td>\n",
       "      <td>q. how covid-19 influenza viruses different?\\n...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>una de les q&amp;a coronaviruses de la pagina web ...</td>\n",
       "      <td>aquesta informacio es basa sobre tot en un art...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>malaikajabali benjaminpdixon we can't forget r...</td>\n",
       "      <td>malaikajabali benjaminpdixon not mention, neve...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>ex-marlboro man dies smoking-related disease http</td>\n",
       "      <td>\"@washingtonpost: ex-marlboro man dies smoking...</td>\n",
       "      <td>553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>holy shit. doritos flavored mountain dew.\\nall...</td>\n",
       "      <td>boogie2988 boogie2988 dewritos boogie2988 what...</td>\n",
       "      <td>554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>banksy account joins cartoonists support charl...</td>\n",
       "      <td>thepoke sadly work though thepoke \"@theazzo: n...</td>\n",
       "      <td>555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>drtedros whowpro whosearo who_europe pahowho w...</td>\n",
       "      <td>drtedros whowpro whosearo who_europe pahowho w...</td>\n",
       "      <td>556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>can covid-19 caught person symptoms?\\nthe main...</td>\n",
       "      <td>however, many people covid-19 experience mild ...</td>\n",
       "      <td>557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>558 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                source  \\\n",
       "0                  how does covid-19 spread? http http   \n",
       "1    brain_warrior if don't believe me, look change...   \n",
       "2    q. how covid-19 influenza viruses similar?\\n#c...   \n",
       "3    una de les q&a coronaviruses de la pagina web ...   \n",
       "4    malaikajabali benjaminpdixon we can't forget r...   \n",
       "..                                                 ...   \n",
       "553  ex-marlboro man dies smoking-related disease http   \n",
       "554  holy shit. doritos flavored mountain dew.\\nall...   \n",
       "555  banksy account joins cartoonists support charl...   \n",
       "556  drtedros whowpro whosearo who_europe pahowho w...   \n",
       "557  can covid-19 caught person symptoms?\\nthe main...   \n",
       "\n",
       "                                                 reply  event_id  \n",
       "0    wcco thanks, wcco! you station i trust media p...         0  \n",
       "1    brain_warrior tell people \"protesting\". brain_...         1  \n",
       "2    q. how covid-19 influenza viruses different?\\n...         2  \n",
       "3    aquesta informacio es basa sobre tot en un art...         3  \n",
       "4    malaikajabali benjaminpdixon not mention, neve...         4  \n",
       "..                                                 ...       ...  \n",
       "553  \"@washingtonpost: ex-marlboro man dies smoking...       553  \n",
       "554  boogie2988 boogie2988 dewritos boogie2988 what...       554  \n",
       "555  thepoke sadly work though thepoke \"@theazzo: n...       555  \n",
       "556  drtedros whowpro whosearo who_europe pahowho w...       556  \n",
       "557  however, many people covid-19 experience mild ...       557  \n",
       "\n",
       "[558 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('train_data.csv',index=False)\n",
    "dev_df.to_csv('dev_data.csv',index=False)\n",
    "test_df.to_csv('test_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel, BertTokenizer, BertModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from nltk import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5. can regularly rinsing nose saline help prevent infection new coronavirus? http 6. do vaccines pneumonia protect new coronavirus? http 7. can spraying alcohol chlorine body kill new coronavirus? chamber http 8. how effective thermal scanners detecting people infected new coronavirus? http 9. can ultraviolet disinfection lamp kill new coronavirus? http 10. are hand dryers effective killing new coronavirus? http 11. the new coronavirus cannot transmitted mosquito bites. http 12. taking hot bath prevent new coronavirus disease http 13. cold weather snow cannot kill new coronavirus. http 14. covid-19 virus transmitted areas hot humid climates http 15. drinking alcohol protect covid-19 dangerous http 16. being able hold breath 10 seconds without coughing feeling discomfort does not mean free coronavirus disease (covid-19) lung disease. http 17. you recover coronavirus disease (covid-19). catching new coronavirus does not mean life. http 18. exposing sun temperatures higher 25c degrees does not prevent coronavirus disease (covid-19) http 19. 5g mobile networks do not spread covid-19 http'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.loc[0, 'reply']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, dataframe, maxlen):\n",
    "        self.dataset = dataframe\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.maxlen = maxlen\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        source = self.dataset.loc[index, 'source']\n",
    "        reply = self.dataset.loc[index,'reply']\n",
    "        #text = self.dataset.loc[index, 'text']\n",
    "        label = self.dataset.loc[index, 'label']\n",
    "        #source = text_preprocess(source)\n",
    "        #reply = text_preprocess(reply)\n",
    "        \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "          source,\n",
    "          reply,\n",
    "          #text,\n",
    "          padding='max_length',\n",
    "          add_special_tokens=True,\n",
    "          return_attention_mask=True,\n",
    "          max_length=self.maxlen,\n",
    "          return_token_type_ids=True,\n",
    "          truncation=True,\n",
    "          )\n",
    "        token_ids = inputs['input_ids']\n",
    "        seg_ids = inputs['token_type_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        tokens_ids_tensor = torch.tensor(token_ids,dtype=torch.long)\n",
    "        attn_mask = torch.tensor(mask, dtype=torch.long)\n",
    "        seg_ids_tensor = torch.tensor(seg_ids, dtype=torch.long)\n",
    "        \n",
    "        return tokens_ids_tensor, attn_mask, seg_ids_tensor, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, dataframe, maxlen):\n",
    "        self.dataset = dataframe\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.maxlen = maxlen\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        source = self.dataset.loc[index, 'source']\n",
    "        reply = self.dataset.loc[index,'reply']\n",
    "        #source = text_preprocess(source)\n",
    "        #reply = text_preprocess(reply)\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "          source,\n",
    "          reply,\n",
    "          padding='max_length',\n",
    "          add_special_tokens=True,\n",
    "          return_attention_mask=True,\n",
    "          max_length=self.maxlen,\n",
    "          return_token_type_ids=True,\n",
    "          truncation=True\n",
    "          )\n",
    "        token_ids = inputs['input_ids']\n",
    "        seg_ids = inputs['token_type_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        tokens_ids_tensor = torch.tensor(token_ids,dtype=torch.long)\n",
    "        attn_mask = torch.tensor(mask, dtype=torch.long)\n",
    "        seg_ids_tensor = torch.tensor(seg_ids, dtype=torch.long)\n",
    "        \n",
    "        return tokens_ids_tensor, attn_mask, seg_ids_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = TweetDataset(train_df, maxlen=120)\n",
    "dev_set = TweetDataset(dev_df, maxlen=120)\n",
    "test_set = TestDataset(test_df, maxlen=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size = 64, num_workers = 2)\n",
    "dev_loader = DataLoader(dev_set, batch_size = 64, num_workers = 2)\n",
    "test_loader = DataLoader(test_set, batch_size = 64, num_workers = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"tristantristantristan/rumor\")\n",
    "\n",
    "class RumourModel(nn.Module):\n",
    "    \n",
    "    def __init__(self,model_name):\n",
    "        super(RumorModel, self).__init__()\n",
    "        #self.MODEL = 'cardiffnlp/twitter-roberta-base-sentiment'\n",
    "        #self.bert_model = AutoModelForSequenceClassification.from_pretrained(self.MODEL)\n",
    "        self.MODEL = 'bert-base-uncased'\n",
    "        self.bert_model = BertModel.from_pretrained(self.MODEL)\n",
    "        #self.bert_model = AutoModel.from_pretrained('cardiffnlp/twitter-roberta-base-2021-124m')\n",
    "        #self.dropout = nn.Dropout(p=0.3)\n",
    "        self.cls_layer = nn.Linear(768,1)\n",
    "\n",
    "    def forward(self, ids, mask, seg_ids):\n",
    "        outputs = self.bert_model(ids,attention_mask=mask,token_type_ids=seg_ids,return_dict=True)\n",
    "        #pooled_output = outputs.pooler_output\n",
    "\n",
    "        #pooled_output = torch.cat(tuple([hidden_states[:,i] for i in [-4, -3, -2, -1]]), dim=-1)\n",
    "        #pooled_output = self.dropout(pooled_output)\n",
    "        # classifier of course has to be 4 * hidden_dim, because we concat 4 layers\n",
    "\n",
    "        #logits = self.cls_layer(pooled_output)\n",
    "\n",
    "        last = outputs.last_hidden_state\n",
    "        cls_rep = last[:,0]\n",
    "\n",
    "        #cls_rep = self.dropout(pooled_output)\n",
    "        logits = self.cls_layer(cls_rep)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu = 0 #gpu ID\n",
    "\n",
    "print(\"Creating the rumour model, initialised with pretrained BERT-BASE parameters...\")\n",
    "net = RumourModel()\n",
    "# net.cuda(gpu) #Enable gpu support for the model\n",
    "print(\"Done creating the runmour model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/l4/7r85t_v944qgzzxzgc4cs2tc0000gn/T/ipykernel_19500/3775186398.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mopti\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "opti = optim.SGD(net.parameters(), lr = 0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train(net, criterion, opti, train_loader, dev_loader, max_eps, gpu):\n",
    "\n",
    "    best_acc = 0\n",
    "    st = time.time()\n",
    "    for ep in tqdm(range(max_eps)):\n",
    "        \n",
    "        net.train()\n",
    "        for it, (seq, attn_masks, seg_ids, labels) in enumerate(train_loader):\n",
    "            #Clear gradients\n",
    "            opti.zero_grad()  \n",
    "            #Converting these to cuda tensors\n",
    "            seq, attn_masks, seg_ids, labels = seq.cuda(gpu), attn_masks.cuda(gpu), seg_ids.cuda(gpu), labels.cuda(gpu)\n",
    "\n",
    "            #Obtaining the logits from the model\n",
    "            logits = net(seq, attn_masks, seg_ids)\n",
    "\n",
    "            #Computing loss\n",
    "            loss = criterion(logits.squeeze(-1), labels.float())\n",
    "\n",
    "            #Backpropagating the gradients\n",
    "            loss.backward()\n",
    "\n",
    "            #Optimization step\n",
    "            opti.step()\n",
    "              \n",
    "            if it % 100 == 0:\n",
    "                \n",
    "                acc = get_accuracy_from_logits(logits, labels)\n",
    "                print(\"Iteration {} of epoch {} complete. Loss: {}; Accuracy: {}; Time taken (s): {}\".format(it, ep, loss.item(), acc, (time.time()-st)))\n",
    "                st = time.time()\n",
    "\n",
    "        \n",
    "        dev_acc, dev_loss = evaluate(net, criterion, dev_loader, gpu)\n",
    "        print(\"Epoch {} complete! Development Accuracy: {}; Development Loss: {}\".format(ep, dev_acc, dev_loss))\n",
    "        if dev_acc > best_acc:\n",
    "            print(\"Best development accuracy improved from {} to {}, saving model...\".format(best_acc, dev_acc))\n",
    "            best_acc = dev_acc\n",
    "            torch.save(net.state_dict(), 'sstcls_{}.dat'.format(ep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_from_logits(logits, labels):\n",
    "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
    "    soft_probs = (probs > 0.5).long()\n",
    "    acc = (soft_probs.squeeze() == labels).float().mean()\n",
    "    return acc\n",
    "\n",
    "def evaluate(net, criterion, dataloader, gpu):\n",
    "    net.eval()\n",
    "\n",
    "    mean_acc, mean_loss = 0, 0\n",
    "    count = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for seq, attn_masks, labels in dataloader:\n",
    "            seq, attn_masks, labels = seq.cuda(gpu), attn_masks.cuda(gpu), labels.cuda(gpu)\n",
    "            logits = net(seq, attn_masks)\n",
    "            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
    "            mean_acc += get_accuracy_from_logits(logits, labels)\n",
    "            count += 1\n",
    "\n",
    "    return mean_acc / count, mean_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 8\n",
    "\n",
    "#fine-tune the model\n",
    "train(net, criterion, opti, train_loader, dev_loader, num_epoch, gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = 0\n",
    "path = '/content/sstcls_0.dat'\n",
    "mode_predict = RumourModel()\n",
    "mode_predict.load_state_dict(torch.load(path))\n",
    "mode_predict.cuda(gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataloader, gpu):\n",
    "    model.eval()\n",
    "\n",
    "    #mean_acc, mean_loss = 0, 0\n",
    "    count = 0\n",
    "    prob_list = []\n",
    "    predicted_label_list = []\n",
    "    with torch.no_grad():\n",
    "        for seq, attn_masks, seg_ids in dataloader:\n",
    "            seq, attn_masks, seg_ids = seq.cuda(gpu), attn_masks.cuda(gpu), seg_ids.cuda(gpu)\n",
    "            logits = model(seq, attn_masks, seg_ids)\n",
    "            probs = torch.sigmoid(logits.unsqueeze(-1))\n",
    "            soft_probs = (probs > 0.5).long()\n",
    "\n",
    "            predicted_label_list.append(soft_probs.squeeze().cpu().detach().numpy())\n",
    "\n",
    "#            acc = (soft_probs.squeeze() == labels)\n",
    " #           prob_list.append(soft_probs)\n",
    "            #predicted_label_list.append(soft_probs.squeeze())\n",
    "            #mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
    "            #mean_acc += get_accuracy_from_logits(logits, labels)\n",
    "            #count += 1\n",
    "\n",
    "    return prob_list, predicted_label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = 0\n",
    "prob_list, predicted_label_list = predict(model_predict, test_loader, gpu )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def output_label(test_df, label_list):\n",
    "    output = label_list[0]\n",
    "    for tensor in label_list[1:]:\n",
    "        #temp_arr = tensor.cpu().detach().numpy()\n",
    "        #print(temp_arr)\n",
    "        output = np.concatenate((output, tensor))\n",
    "        test_df['label'] = output\n",
    "        #output_df = test_df.loc[['problem_num','label']]\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = output_label(test_df, predicted_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = output_df[['event_id','label']]\n",
    "output_df.columns = ['Id','Predicted']\n",
    "output_df\n",
    "output_df.to_csv('out.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
