{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biases in Pretrained Embeddings\n",
    "\n",
    "In this notebook, we'll attempt to build a sentiment classifier, first using pretrained word embeddings (GloVe) and then next using BERT (from Week 7's workshop), and see if these pretrained embeddings/models inherently contain any biases.\n",
    "\n",
    "What is GloVe embeddings? They are word embeddings like Word2Vec, but implemented differently. If you're interested to read more about GloVe, you can find more information [here](https://nlp.stanford.edu/projects/glove/). The crucial thing to note here is that GloVe embeddings are trained on Google News and Common Crawl web data, and so the embeddings themselves are likely to capture common stereotypes and biases in our culture.\n",
    "\n",
    "First, let's upload the GloVe embeddings (\"13-glove.6B.50d.txt\") to your colab instance.\n",
    "\n",
    "Refresher:\n",
    "\n",
    "1. To upload files, click the folder icon on the left, and click the \"upload\" icon to choose files from your local drive (you can also drag and drop files to upload them). Once the files are uploaded, you should see them appearing in the file system.\n",
    "\n",
    "2. Don't forget to enable GPU on the colab notebook. We can do this by going to \"Runtime $>$ Change runtime type\" and selecting \"GPU\" as the hardware accelerator. Click save."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get started. Let's load some libraries that we'll be using for building the first sentiment classifier using the GloVe embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import re\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll define a function to load the embeddings using the text file. The format is pretty self-explanatory if you view the embeddings file: it's just one line for each word.\n",
    "\n",
    "Note: the loading process might take a couple seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(filename):\n",
    "    \"\"\"\n",
    "    Load a DataFrame from the generalized text format used by word2vec, GloVe,\n",
    "    fastText, and ConceptNet Numberbatch. The main point where they differ is\n",
    "    whether there is an initial line with the dimensions of the matrix.\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    rows = []\n",
    "    with open(filename, encoding='utf-8') as infile:\n",
    "        for i, line in enumerate(infile):\n",
    "            items = line.rstrip().split(' ')\n",
    "            if len(items) == 2:\n",
    "                # This is a header row giving the shape of the matrix\n",
    "                continue\n",
    "            labels.append(items[0])\n",
    "            values = np.array([float(x) for x in items[1:]], 'f')\n",
    "            rows.append(values)\n",
    "    \n",
    "    arr = np.vstack(rows)\n",
    "    return pd.DataFrame(arr, index=labels, dtype='f')\n",
    "\n",
    "embeddings = load_embeddings('13-glove.6B.50d.txt')\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to build a sentiment classifier is to use a sentiment lexicon: a dictionary that contains positive and negative words. There are many sentiment lexicons you could use, but we'll be using the [Opinion Lexicon](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon). Download them (`13-positive-words.txt` and `13-negative-words.txt` from Canvas) and put them in the same directory of this notebook. Once that's done, we'll load the lexicon using the function defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lexicon(filename):\n",
    "    \"\"\"\n",
    "    Load a file from Bing Liu's sentiment lexicon\n",
    "    (https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html), containing\n",
    "    English words in Latin-1 encoding.\n",
    "    \n",
    "    One file contains a list of positive words, and the other contains\n",
    "    a list of negative words. The files contain comment lines starting\n",
    "    with ';' and blank lines, which should be skipped.\n",
    "    \"\"\"\n",
    "    lexicon = []\n",
    "    with open(filename, encoding='latin-1') as infile:\n",
    "        for line in infile:\n",
    "            line = line.rstrip()\n",
    "            if line and not line.startswith(';'):\n",
    "                lexicon.append(line)\n",
    "    return lexicon\n",
    "\n",
    "pos_words = load_lexicon('13-positive-words.txt')\n",
    "neg_words = load_lexicon('13-negative-words.txt')\n",
    "\n",
    "print(pos_words[:30])\n",
    "print(neg_words[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we build a sentiment classifier using word embeddings? We'll take a very simple approach, where we embed the positive and negative words using GloVe embeddings, and then train a logistic regression model that predict their sentiments based on their embeddings. Once trained, we can then apply the logistic regression to all other words not part of the lexicon that have a corresponding GloVe embeddings to predict their sentiments.\n",
    "\n",
    "Note: `index.intersection()` is used to only words that are in the GloVe vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_vectors = embeddings.loc[embeddings.index.intersection(pos_words)]\n",
    "neg_vectors = embeddings.loc[embeddings.index.intersection(neg_words)]\n",
    "\n",
    "pos_vectors.sort_index(inplace = True)\n",
    "neg_vectors.sort_index(inplace = True)\n",
    "\n",
    "pos_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make arrays of the desired inputs and outputs. The inputs are the embeddings, and the outputs are 1 for positive words and -1 for negative words. We also make sure to keep track of the words they’re labeled with, so we can interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = pd.concat([pos_vectors, neg_vectors])\n",
    "targets = np.array([1 for entry in pos_vectors.index] + [-1 for entry in neg_vectors.index])\n",
    "labels = list(pos_vectors.index) + list(neg_vectors.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we split the input vectors, output values, and labels into training and test data, with 10% of the data used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors, test_vectors, train_targets, test_targets, train_labels, test_labels = \\\n",
    "    train_test_split(vectors, targets, labels, test_size=0.1, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make our classifier, and train it by running the training vectors through it for 100 iterations. We use a logistic function as the loss as we're building a logistic regression model. The resulting classifier should output the probability that a word is positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SGDClassifier(loss='log', random_state=0, max_iter=100)\n",
    "model.fit(train_vectors, train_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the classifier on the test vectors. It predicts the correct sentiment for sentiment words outside of its training data with around 88% accuracy. Not bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(model.predict(test_vectors), test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s define a function that we can use to see the sentiment that this classifier predicts for particular words, then use it to see some examples of its predictions on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vecs_to_sentiment(vecs):\n",
    "    # predict_log_proba gives the log probability for each class\n",
    "    predictions = model.predict_log_proba(vecs)\n",
    "\n",
    "    # To see an overall positive vs. negative classification in one number,\n",
    "    # we take the log probability of positive sentiment minus the log\n",
    "    # probability of negative sentiment.\n",
    "    return predictions[:, 1] - predictions[:, 0]\n",
    "\n",
    "\n",
    "def words_to_sentiment(words):\n",
    "    vecs = embeddings.loc[embeddings.index.intersection(words)]\n",
    "    log_odds = vecs_to_sentiment(vecs)\n",
    "    return pd.DataFrame({'sentiment': log_odds}, index=vecs.index)\n",
    "\n",
    "\n",
    "# Show 20 examples from the test set\n",
    "words_to_sentiment(test_labels).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More than the accuracy number, this convinces us that the classifier is working. We can see that the classifier has learned to generalize sentiment to words outside of its training data. Note that the returned sentiment here is the result of `logprob(positive_class) - logprob(negative_class)`, and a positive value indicates positive sentiment and negative value indicates negative sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're reasonably happy with the classifier, we'll extend it to classify sentiment for a sentence. We can do so by simply computing the sentiment for each word in the sentence, and then taking the mean sentiment over all words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "TOKEN_RE = re.compile(r\"\\w.*?\\b\")\n",
    "# The regex above finds tokens that start with a word-like character (\\w), and continues\n",
    "# matching characters (.+?) until the next word break (\\b). It's a relatively simple\n",
    "# expression that manages to extract something very much like words from text.\n",
    "\n",
    "\n",
    "def text_to_sentiment(text):\n",
    "    tokens = [token.casefold() for token in TOKEN_RE.findall(text)]\n",
    "    sentiments = words_to_sentiment(tokens)\n",
    "    return sentiments['sentiment'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test on some example sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_to_sentiment(\"this example is pretty cool\"))\n",
    "print(text_to_sentiment(\"this example is okay\"))\n",
    "print(text_to_sentiment(\"meh, this example sucks\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results look pretty reasonable. Let's try more examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_to_sentiment(\"Let's go get Italian food\"))\n",
    "print(text_to_sentiment(\"Let's go get Chinese food\"))\n",
    "print(text_to_sentiment(\"Let's go get Mexican food\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. Mexican food seem to be associated with a negative sentiment. Let's try some names.\n",
    "\n",
    "Note: there will be randomness in terms of the output. Do not be alarmed if you see different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_to_sentiment(\"My name is Emily\"))\n",
    "print(text_to_sentiment(\"My name is Heather\"))\n",
    "print(text_to_sentiment(\"My name is Yvette\"))\n",
    "print(text_to_sentiment(\"My name is Yasin\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the system has widely different sentiments with people's names. This is a little worrying. Did we just build a _racist_ sentiment classifier?\n",
    "\n",
    "Note: there will be randomness in terms of the output. Do not be alarmed if you see different results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's measure this bias with a bit more rigour.\n",
    "\n",
    "Below we have four lists of names that tend to reflect different ethnic backgrounds. The first two are lists of predominantly “Caucasian” and “African American” names adapted from [this paper](https://arxiv.org/pdf/1608.07187.pdf). We've also added typically Hispanic names, as well as Muslim names that come from Arabic or Urdu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAMES_BY_ETHNICITY = {\n",
    "    # The first two lists are from the Caliskan et al. appendix describing the\n",
    "    # Word Embedding Association Test.\n",
    "    'Caucasian': [\n",
    "        'Adam', 'Chip', 'Harry', 'Josh', 'Roger', 'Alan', 'Frank', 'Ian', 'Justin',\n",
    "        'Ryan', 'Andrew', 'Fred', 'Jack', 'Matthew', 'Stephen', 'Brad', 'Greg', 'Jed',\n",
    "        'Paul', 'Todd', 'Brandon', 'Hank', 'Jonathan', 'Peter', 'Wilbur', 'Amanda',\n",
    "        'Courtney', 'Heather', 'Melanie', 'Sara', 'Amber', 'Crystal', 'Katie',\n",
    "        'Meredith', 'Shannon', 'Betsy', 'Donna', 'Kristin', 'Nancy', 'Stephanie',\n",
    "        'Bobbie-Sue', 'Ellen', 'Lauren', 'Peggy', 'Sue-Ellen', 'Colleen', 'Emily',\n",
    "        'Megan', 'Rachel', 'Wendy'\n",
    "    ],\n",
    "\n",
    "    'African American': [\n",
    "        'Alonzo', 'Jamel', 'Lerone', 'Percell', 'Theo', 'Alphonse', 'Jerome',\n",
    "        'Leroy', 'Rasaan', 'Torrance', 'Darnell', 'Lamar', 'Lionel', 'Rashaun',\n",
    "        'Tyree', 'Deion', 'Lamont', 'Malik', 'Terrence', 'Tyrone', 'Everol',\n",
    "        'Lavon', 'Marcellus', 'Terryl', 'Wardell', 'Aiesha', 'Lashelle', 'Nichelle',\n",
    "        'Shereen', 'Temeka', 'Ebony', 'Latisha', 'Shaniqua', 'Tameisha', 'Teretha',\n",
    "        'Jasmine', 'Latonya', 'Shanise', 'Tanisha', 'Tia', 'Lakisha', 'Latoya',\n",
    "        'Sharise', 'Tashika', 'Yolanda', 'Lashandra', 'Malika', 'Shavonn',\n",
    "        'Tawanda', 'Yvette'\n",
    "    ],\n",
    "    \n",
    "    # This list comes from statistics about common Hispanic-origin names in the US.\n",
    "    'Hispanic': [\n",
    "        'Juan', 'José', 'Miguel', 'Luís', 'Jorge', 'Santiago', 'Matías', 'Sebastián',\n",
    "        'Mateo', 'Nicolás', 'Alejandro', 'Samuel', 'Diego', 'Daniel', 'Tomás',\n",
    "        'Juana', 'Ana', 'Luisa', 'María', 'Elena', 'Sofía', 'Isabella', 'Valentina',\n",
    "        'Camila', 'Valeria', 'Ximena', 'Luciana', 'Mariana', 'Victoria', 'Martina'\n",
    "    ],\n",
    "    \n",
    "    # This list is compiled from baby-name sites for common Muslim names,\n",
    "    # as spelled in English.\n",
    "    # Note: the following list potentially conflates religion and ethnicity and so it isn't\n",
    "    # perfect.\n",
    "    'Arab/Muslim': [\n",
    "        'Mohammed', 'Omar', 'Ahmed', 'Ali', 'Youssef', 'Abdullah', 'Yasin', 'Hamza',\n",
    "        'Ayaan', 'Syed', 'Rishaan', 'Samar', 'Ahmad', 'Zikri', 'Rayyan', 'Mariam',\n",
    "        'Jana', 'Malak', 'Salma', 'Nour', 'Lian', 'Fatima', 'Ayesha', 'Zahra', 'Sana',\n",
    "        'Zara', 'Alya', 'Shaista', 'Zoya', 'Yasmin'\n",
    "    ],\n",
    "    \n",
    "    # This list uses some of the most common Chinese given names\n",
    "    # (https://en.wikipedia.org/wiki/Chinese_given_name)\n",
    "    'Chinese': [\n",
    "        'Wei', 'Fang', 'Xiu Ying', 'Na', 'Min', 'Jing', 'Li', 'Qiang', 'Lei',\n",
    "        'Yang', 'Jie', 'Jun', 'Yong', 'Yan', \"Chao\", \"Tao\", \"Juan\", \"Han\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name_sentiment_table():\n",
    "    frames = []\n",
    "    for group, name_list in sorted(NAMES_BY_ETHNICITY.items()):\n",
    "        lower_names = [name.lower() for name in name_list]\n",
    "        sentiments = words_to_sentiment(lower_names)\n",
    "        sentiments['group'] = group\n",
    "        frames.append(sentiments)\n",
    "\n",
    "    # Put together the data we got from each ethnic group into one big table\n",
    "    return pd.concat(frames)\n",
    "\n",
    "name_sentiments = name_sentiment_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_sentiments.iloc[::25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#TODO: code below could possibly be simplified here\n",
    "\n",
    "caucasian_sentiments = name_sentiments[name_sentiments[\"group\"] == \"Caucasian\"][\"sentiment\"].tolist()\n",
    "africanamerican_sentiments = name_sentiments[name_sentiments[\"group\"] == \"African American\"][\"sentiment\"].tolist()\n",
    "hispanic_sentiments = name_sentiments[name_sentiments[\"group\"] == \"Hispanic\"][\"sentiment\"].tolist()\n",
    "arab_sentiments = name_sentiments[name_sentiments[\"group\"] == \"Arab/Muslim\"][\"sentiment\"].tolist()\n",
    "chinese_sentiments = name_sentiments[name_sentiments[\"group\"] == \"Chinese\"][\"sentiment\"].tolist()\n",
    "\n",
    "x = name_sentiments[\"sentiment\"].tolist()\n",
    "y= name_sentiments[\"group\"].tolist()\n",
    "\n",
    "plt.boxplot([caucasian_sentiments, africanamerican_sentiments, hispanic_sentiments, arab_sentiments, chinese_sentiments],\n",
    "           labels = [\"Caucasian\", \"African American\", \"Hispanic\", \"Arab/Muslim\", \"Chinese\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the mean sentiment over these different ethnic groups, it's pretty clear that Black names are on average associated with negative sentiment, and so are Arab/Muslim and Chinese names (although not quite as negative).\n",
    "\n",
    "Why is this happening? Where is the source of bias? It's not from the sentiment lexicon, because it doesn't include any names. The source of bias comes from the pretrained GloVe word embeddings, which are trained using news and web data. As the data encodes biases and stereotypes that reflect our worldview, the sentiment classifier we built ultimately reflects that. It is perhaps impossible to have create perfectly neutral models or datasets, but the point here is **awareness**: that as engineers we should at least be wary of the biases of the data/models we develop, and documenting them in a way such that users/companies that use our systems know their limitations or weaknesses. Although we have yet to explore how to reduce bias, this exercise of building awareness constitutes the first step towards building ethical AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
