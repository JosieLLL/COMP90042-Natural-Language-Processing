{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributional Semantics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, we'll be using the 500 document Brown corpus included in NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is divided up into two independent parts: the first uses PMI for distinguishing good collocations, and the second involves building a vector space model.\n",
    "\n",
    "For the PMI portion, we'll use a function which extracts the information we need for a particular two word collocation, namely counts of each word individually, counts of the collocation, and the total number of word tokens in the corpus, and then calculates the PMI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_PMI_for_collocation_brown(word1,word2):\n",
    "    word1_count = 0\n",
    "    word2_count = 0\n",
    "    both_count = 0\n",
    "    total_count = 0.0 # so that division results in a float\n",
    "    for sent in brown.sents():\n",
    "        sent = [word.lower() for word in sent]\n",
    "        for i in range(len(sent)):\n",
    "            total_count += 1\n",
    "            if sent[i] == word1:\n",
    "                word1_count += 1\n",
    "                if i < len(sent) - 1 and sent[i + 1] == word2:\n",
    "                    both_count += 1\n",
    "            elif sent[i] == word2:\n",
    "                word2_count += 1\n",
    "    return math.log((both_count/total_count)/((word1_count/total_count)*(word2_count/total_count)), 2)\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in a typical use case, we probably wouldn't do it this way, since we'd likely want to calculate PMI across many different words, and collecting the statisitcs for this can be done in a single pass across the corpus for all words, and then the PMI calculated in a separate function. Anyway, let's compare the PMI for two phrases, \"hard work\" and \"some work\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.237244531670497\n",
      "1.9135320271049516\n"
     ]
    }
   ],
   "source": [
    "print(get_PMI_for_collocation_brown(\"hard\",\"work\"))\n",
    "print(get_PMI_for_collocation_brown(\"some\",\"work\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on PMI, \"hard work\" appears to be a much better collocation than \"some work\", which matches our intuition. Go ahead and try out this out some other collocations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second part of the notebook, let's first create a sparse document-term matrix, using sci-kit learn. We'll then apply tf-idf weighting and SVD to learn word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 49)\t1.0\n",
      "  (0, 58)\t1.0\n",
      "  (0, 169)\t1.0\n",
      "  (0, 181)\t1.0\n",
      "  (0, 205)\t1.0\n",
      "  (0, 238)\t1.0\n",
      "  (0, 322)\t33.0\n",
      "  (0, 373)\t3.0\n",
      "  (0, 374)\t3.0\n",
      "  (0, 393)\t87.0\n",
      "  (0, 395)\t4.0\n",
      "  (0, 405)\t88.0\n",
      "  (0, 454)\t4.0\n",
      "  (0, 465)\t1.0\n",
      "  (0, 695)\t1.0\n",
      "  (0, 720)\t1.0\n",
      "  (0, 939)\t1.0\n",
      "  (0, 1087)\t1.0\n",
      "  (0, 1103)\t1.0\n",
      "  (0, 1123)\t1.0\n",
      "  (0, 1159)\t1.0\n",
      "  (0, 1170)\t1.0\n",
      "  (0, 1173)\t1.0\n",
      "  (0, 1200)\t3.0\n",
      "  (0, 1451)\t1.0\n",
      "  :\t:\n",
      "  (499, 49161)\t1.0\n",
      "  (499, 49164)\t1.0\n",
      "  (499, 49242)\t1.0\n",
      "  (499, 49253)\t1.0\n",
      "  (499, 49275)\t1.0\n",
      "  (499, 49301)\t1.0\n",
      "  (499, 49313)\t1.0\n",
      "  (499, 49369)\t1.0\n",
      "  (499, 49385)\t1.0\n",
      "  (499, 49386)\t4.0\n",
      "  (499, 49390)\t2.0\n",
      "  (499, 49410)\t2.0\n",
      "  (499, 49446)\t1.0\n",
      "  (499, 49576)\t1.0\n",
      "  (499, 49590)\t1.0\n",
      "  (499, 49613)\t3.0\n",
      "  (499, 49691)\t42.0\n",
      "  (499, 49694)\t3.0\n",
      "  (499, 49697)\t3.0\n",
      "  (499, 49698)\t1.0\n",
      "  (499, 49707)\t17.0\n",
      "  (499, 49708)\t1.0\n",
      "  (499, 49710)\t4.0\n",
      "  (499, 49711)\t1.0\n",
      "  (499, 49797)\t1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "def get_BOW(text):\n",
    "    BOW = {}\n",
    "    for word in text:\n",
    "        BOW[word.lower()] = BOW.get(word.lower(),0) + 1\n",
    "    return BOW\n",
    "\n",
    "texts = []\n",
    "for fileid in brown.fileids():\n",
    "    texts.append(get_BOW(brown.words(fileid)))\n",
    "\n",
    "vectorizer = DictVectorizer()\n",
    "brown_matrix = vectorizer.fit_transform(texts)\n",
    "print(brown_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our matrix is sparse: for instance, columns 0-48 in row 0 are empty, and are just left out, only the rows and columns with values other than zeros are displayed\n",
    "\n",
    "Rather than removing stopwords as we did for text classification, let's add some idf weighting to this matrix. Scikit-learn has a built-in tf-idf transformer for just this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 49646)\t1.7298111649315369\n",
      "  (0, 49613)\t1.3681693233644676\n",
      "  (0, 49596)\t3.7066318654255337\n",
      "  (0, 49386)\t9.98833379406486\n",
      "  (0, 49378)\t8.731629015654066\n",
      "  (0, 49313)\t2.62964061975162\n",
      "  (0, 49301)\t7.374075931214787\n",
      "  (0, 49292)\t2.184170177029756\n",
      "  (0, 49224)\t3.385966701933097\n",
      "  (0, 49147)\t6.0\n",
      "  (0, 49041)\t3.407945608651872\n",
      "  (0, 49003)\t22.210096880912054\n",
      "  (0, 49001)\t5.741605353137016\n",
      "  (0, 48990)\t16.84677293625242\n",
      "  (0, 48951)\t4.7297014486341915\n",
      "  (0, 48950)\t4.939351940117883\n",
      "  (0, 48932)\t3.9565115604007097\n",
      "  (0, 48867)\t7.046120322868667\n",
      "  (0, 48777)\t1.41855034765682\n",
      "  (0, 48771)\t13.694210097452498\n",
      "  (0, 48769)\t6.236428984115791\n",
      "  (0, 48753)\t1.2957142441490452\n",
      "  (0, 48749)\t3.1984194075136347\n",
      "  (0, 48720)\t1.1648746431902341\n",
      "  (0, 48670)\t2.1974319458783156\n",
      "  :\t:\n",
      "  (499, 2710)\t3.120263536200091\n",
      "  (499, 2688)\t2.04412410338404\n",
      "  (499, 2670)\t3.9565115604007097\n",
      "  (499, 2611)\t4.270169119255751\n",
      "  (499, 2468)\t6.521460917862246\n",
      "  (499, 2439)\t4.170085660698769\n",
      "  (499, 2415)\t4.122633007848826\n",
      "  (499, 2413)\t2.320337504305643\n",
      "  (499, 2388)\t2.096614286005437\n",
      "  (499, 2358)\t6.115995809754082\n",
      "  (499, 2290)\t61.0\n",
      "  (499, 2289)\t7.5533024513831695\n",
      "  (499, 2286)\t11.156201344558639\n",
      "  (499, 2285)\t20.714812015222506\n",
      "  (499, 2283)\t1.2256466815323281\n",
      "  (499, 1345)\t6.521460917862246\n",
      "  (499, 1141)\t4.506557897319982\n",
      "  (499, 405)\t83.0\n",
      "  (499, 395)\t12.710333931244342\n",
      "  (499, 393)\t188.0\n",
      "  (499, 374)\t4.0872168559431525\n",
      "  (499, 373)\t4.095849955425997\n",
      "  (499, 354)\t7.214608098422191\n",
      "  (499, 322)\t7.538167310351703\n",
      "  (499, 320)\t3.4769384801388235\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "transformer = TfidfTransformer(smooth_idf=False,norm=None)\n",
    "\n",
    "brown_matrix_tfidf = transformer.fit_transform(brown_matrix)\n",
    "\n",
    "print(brown_matrix_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's apply SVD. Scikit-learn does not expose the internal details of the decomposition, we just use the [TruncatedSVD class](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html) directly get a matrix with k dimensions. Since the Brown corpus is a fairly small corpus, we'll do k=10. Note that we'll first transpose the document-term sparse matrix to a term-document matrix before we apply SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49815, 10)\n",
      "[[ 1.46529922e+02 -1.56578300e+02  3.77295895e+01 ...  1.87574330e+01\n",
      "   5.17826940e+00 -1.32357467e+01]\n",
      " [ 6.10797743e-01  6.77336542e-01 -2.04392054e-01 ... -1.02796238e+00\n",
      "  -1.14385161e-01 -1.12871217e+00]\n",
      " [ 1.00411586e+00  1.99456979e-01 -1.25054329e-01 ...  1.14578446e+00\n",
      "  -4.14250674e-01 -1.68706426e-01]\n",
      " ...\n",
      " [ 3.26612758e-01  2.53370725e-01 -2.71177861e-01 ...  2.51508282e-01\n",
      "   1.31093947e-01  1.59715022e-01]\n",
      " [ 6.35382477e-01  7.12100488e-01 -2.82140022e-02 ...  6.70060518e-01\n",
      "   1.78645267e-01  3.52829119e-01]\n",
      " [ 3.27037764e-01  7.38765531e-01  2.09243078e+00 ... -2.95536854e-01\n",
      "  -3.95585989e-01 -1.02777409e-02]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "#dimension of brown_matrix_tfidf = num_documents x num_vocab\n",
    "#dimension of brown_matrix_tfidf_transposed = num_vocab x num_documents\n",
    "\n",
    "brown_matrix_tfidf_transposed = csr_matrix(brown_matrix_tfidf).transpose()\n",
    "\n",
    "svd = TruncatedSVD(n_components=10)\n",
    "brown_matrix_lowrank = svd.fit_transform(brown_matrix_tfidf_transposed)\n",
    "\n",
    "print(brown_matrix_lowrank.shape)\n",
    "print(brown_matrix_lowrank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned matrix corresponds to the transformed term/word matrix, $U \\Sigma$, after SVD factorisation, $X \\approx U \\Sigma V^T$, applied to `brown_matrix_tfidf_transposed`, as $X$. Note that the resulting matrix is not sparse.\n",
    "\n",
    "The last thing we'll do is to compare some words and see if their similarity fits our intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8095752240062064\n",
      "0.14326323746458713\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "v1 = brown_matrix_lowrank[vectorizer.vocabulary_[\"medical\"]]\n",
    "v2 = brown_matrix_lowrank[vectorizer.vocabulary_[\"health\"]]\n",
    "v3 = brown_matrix_lowrank[vectorizer.vocabulary_[\"gun\"]]\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    return np.dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "print(cos_sim(v1, v2))\n",
    "print(cos_sim(v1, v3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There'll be some variability to the exact cosine similarity values that you'll get (feel free to re-run SVD and check this), but hopefully you should find that _medical_ and _health_ is more closely related to each other than _medical_ and _gun_.\n",
    "\n",
    "Next let's try _information_, _retrieval_ and _science_!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4883440848184145\n",
      "0.43337516485029776\n"
     ]
    }
   ],
   "source": [
    "v1 = brown_matrix_lowrank[vectorizer.vocabulary_[\"information\"]]\n",
    "v2 = brown_matrix_lowrank[vectorizer.vocabulary_[\"retrieval\"]]\n",
    "v3 = brown_matrix_lowrank[vectorizer.vocabulary_[\"science\"]]\n",
    "\n",
    "print(cos_sim(v1, v2))\n",
    "print(cos_sim(v1, v3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did you find? Did you get similar results to Discussion Q2 in the worksheet? (you might want to re-run SVD and see if you find contradicting results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
