{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Lexical Semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will use NLTK to access WordNet, look at some senses and lexical relations, and find paths between words. First, let's load NLTK and make sure WordNet is accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\tThis is the README file for WordNet 3.0\n",
      "\n",
      "1. About WordNet\n",
      "\n",
      "WordNet was developed at Princeton University's Cognitive Science\n",
      "Laboratory under the direction of George Miller, James S. McDonnell\n",
      "Distinguished University Professor of Psychology, Emeritus. Over the\n",
      "years many linguists, lexicographers, students, and software engineers\n",
      "have contributed to the project.\n",
      "\n",
      "WordNet is an online lexical reference system.  Word forms in WordNet\n",
      "are represented in their familiar orthography; word meanings are\n",
      "represented by synonym sets (synsets) - lists of synonymous word forms\n",
      "that are interchangeable in some context.  Two kinds of relations are\n",
      "recognized: lexical and semantic.  Lexical relations hold between word\n",
      "forms; semantic relations hold between word meanings.  \n",
      "\n",
      "To learn more about WordNet, the book \"WordNet: An Electronic Lexical\n",
      "Database,\" containing an updated version of \"Five Papers on WordNet\"\n",
      "and additional papers by WordNet users, is available from MIT Press:\n",
      "\n",
      "    http://mitpress.mit.edu/book-home.tcl?isbn=026206197X\n",
      "\n",
      "2. The WordNet Web Site\n",
      "\n",
      "We maintain a Web site at:\n",
      "\n",
      "http://wordnet.princeton.edu\n",
      "\n",
      "Information about WordNet, access to our online interface, and the\n",
      "various WordNet packages that you can download are available from our\n",
      "web site.  All of the software documentation is available online, as\n",
      "well as a FAQ.  On this site we also have information about other\n",
      "applications that use WordNet. If you have an application that you\n",
      "would like included, please send e-mail to the above address.\n",
      "\n",
      "3. Contacting Us\n",
      "\n",
      "Ongoing deveopment work and WordNet related projects are done by a\n",
      "small group of researchers, lexicographers, and systems programmers.\n",
      "Since our resources are VERY limited, we request that you please\n",
      "confine correspondence to WordNet topics only.  Please check the\n",
      "documentation, FAQ, and other resources for the answer to your\n",
      "question or problem before contacting us.\n",
      "\n",
      "If you have trouble installing or downloading WordNet, have a bug to\n",
      "report, or any other problem, please refer to the online FAQ file\n",
      "first.  If you can heal thyself, please do so.  The FAQ will be\n",
      "updated over time.  And if you do find a previously unreported\n",
      "problem, please use our Bug Report Form:\n",
      "\n",
      "http://wordnet.princeton.edu/cgi-bin/bugsubmit.pl\n",
      "\n",
      "When reporting a problem, please be as specific as possible, stating\n",
      "the computer platform you are using, which interface you are using,\n",
      "and the exact error.  The more details you can provide, the more\n",
      "likely it is that you will get an answer. \n",
      "\n",
      "There is a WordNet user discussion group mailing list that we invite\n",
      "our users to join.  Users use this list to ask questions of one\n",
      "another, announce extensions to WordNet that they've developed, and\n",
      "other topics of general usefulness to the user community.\n",
      "\n",
      "Information on joining the user discussion list, reporting bugs and other\n",
      "contact information is in found on our website at:\n",
      "\n",
      "http://wordnet.princeton.edu/contact\n",
      "\n",
      "4. Current Release\n",
      "\n",
      "WordNet Version 3.0 is the latest version available for download.  Two\n",
      "basic database packages are available - one for Windows and one for\n",
      "Unix platforms (including Mac OS X).  See the file ChangeLog (Unix) or\n",
      "CHANGES.txt (Windows) for a list of changes from previous versions.\n",
      "\n",
      "WordNet packages can either be downloaded from our web site via:\n",
      "\n",
      "http://wordnet.princeton.edu/obtain\n",
      "\n",
      "The Windows package is a self-extracting archive that installs itself\n",
      "when you double-click on it.\n",
      "\n",
      "Beginning with Version 2.1, we changed the Unix package to a GNU Autotools\n",
      "package.  The WordNet browser makes use of the open source Tcl and Tk\n",
      "packages. Many systems come with either or both pre-installed.  If\n",
      "your system doesn't (some systems have Tcl installed, but not Tk)\n",
      "Tcl/Tk can be downloaded from:\n",
      "\n",
      "http://www.tcl.tk/ \n",
      "\n",
      "Tcl and Tk must be installed BEFORE you compile WordNet. You must also\n",
      "have a C compiler before installing Tcl/Tk or WordNet.  WordNet has\n",
      "been built and tested with the GNU gcc compiler.  This is\n",
      "pre-installed on most Unix systems, and can be downloaded from:\n",
      "\n",
      "http://gcc.gnu.org/\n",
      "\n",
      "See the file INSTALL for detailed WordNet installation instructions.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "print(wn.readme(lang=\"eng\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in lecture, the main nodes in WordNet are synsets, not words. Given any word, we can access relevant synsets using the synsets commands. We can optionally limit to a particular word category (n = noun, v = verb, a = adjective, r = adverb). For each of the synsets of the word type \"class\", let's look at their definition, their corresponding lemmas, an example of their usage, and their hypernyms (often only one, but can be multiple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book.n.01\n",
      "a written work or composition that has been published (printed on pages bound together)\n",
      "['book']\n",
      "['I am reading a good book on economics']\n",
      "[Synset('publication.n.01')]\n",
      "-------\n",
      "book.n.02\n",
      "physical objects consisting of a number of pages bound together\n",
      "['book', 'volume']\n",
      "['he used a large book as a doorstop']\n",
      "[Synset('product.n.02')]\n",
      "-------\n",
      "record.n.05\n",
      "a compilation of the known facts regarding something or someone\n",
      "['record', 'record_book', 'book']\n",
      "[\"Al Smith used to say, `Let's look at the record'\", 'his name is in all the record books']\n",
      "[Synset('fact.n.02')]\n",
      "-------\n",
      "script.n.01\n",
      "a written version of a play or other dramatic composition; used in preparing for a performance\n",
      "['script', 'book', 'playscript']\n",
      "[]\n",
      "[Synset('dramatic_composition.n.01')]\n",
      "-------\n",
      "ledger.n.01\n",
      "a record in which commercial accounts are recorded\n",
      "['ledger', 'leger', 'account_book', 'book_of_account', 'book']\n",
      "['they got a subpoena to examine our books']\n",
      "[Synset('record.n.07')]\n",
      "-------\n",
      "book.n.06\n",
      "a collection of playing cards satisfying the rules of a card game\n",
      "['book']\n",
      "[]\n",
      "[Synset('collection.n.01')]\n",
      "-------\n",
      "book.n.07\n",
      "a collection of rules or prescribed standards on the basis of which decisions are made\n",
      "['book', 'rule_book']\n",
      "['they run things by the book around here']\n",
      "[Synset('collection.n.01')]\n",
      "-------\n",
      "koran.n.01\n",
      "the sacred writings of Islam revealed by God to the prophet Muhammad during his life at Mecca and Medina\n",
      "['Koran', 'Quran', \"al-Qur'an\", 'Book']\n",
      "[]\n",
      "[]\n",
      "-------\n",
      "bible.n.01\n",
      "the sacred writings of the Christian religions\n",
      "['Bible', 'Christian_Bible', 'Book', 'Good_Book', 'Holy_Scripture', 'Holy_Writ', 'Scripture', 'Word_of_God', 'Word']\n",
      "['he went to carry the Word to the heathen']\n",
      "[Synset('sacred_text.n.01')]\n",
      "-------\n",
      "book.n.10\n",
      "a major division of a long written composition\n",
      "['book']\n",
      "['the book of Isaiah']\n",
      "[Synset('section.n.01')]\n",
      "-------\n",
      "book.n.11\n",
      "a number of sheets (ticket or stamps etc.) bound together on one edge\n",
      "['book']\n",
      "['he bought a book of stamps']\n",
      "[Synset('product.n.02')]\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "for synset in wn.synsets(\"book\",\"n\"):\n",
    "    print(synset.name())\n",
    "    print(synset.definition())\n",
    "    print(synset.lemma_names())\n",
    "    print(synset.examples())\n",
    "    print(synset.hypernyms())\n",
    "    print(\"-------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here why WordNet is sometimes seen as too fine-grained, particularly for word sense disambiguation; several of these senses are closely related to each other in meaning. In any case, once we know its name, we can access a particular synset with the synset command, and look at other relationships, such as hyponyms; Note that meronyms and holonyms come in three types: part, member or substance, though we'll only look at part here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('album.n.02'), Synset('coffee-table_book.n.01'), Synset('folio.n.03'), Synset('hardback.n.01'), Synset('journal.n.04'), Synset('notebook.n.01'), Synset('novel.n.02'), Synset('order_book.n.02'), Synset('paperback_book.n.01'), Synset('picture_book.n.01'), Synset('sketchbook.n.01')]\n",
      "[Synset('binding.n.05'), Synset('fore_edge.n.01'), Synset('spine.n.04')]\n",
      "[Synset('text.n.01')]\n"
     ]
    }
   ],
   "source": [
    "print(wn.synset(\"book.n.02\").hyponyms())\n",
    "print(wn.synset(\"book.n.02\").part_meronyms())\n",
    "print(wn.synset(\"book.n.10\").part_holonyms()) # \"book\" meaning a division of a text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each synset has a set of lemmas associated with it. Since antonyms are often specific to the word form, they are defined on lemmas, not synsets. Another function, derivationally_related_forms gives other lemmas which are related by derivational morphology, though this is not comprehensive. Finally, lemmas have a count associated with them, derived from a sense tagged corpus: these can be used to identify which senses of a word are more common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('happy.a.01')\n",
      "[Lemma('unhappy.a.01.unhappy')]\n",
      "[Lemma('happiness.n.01.happiness'), Lemma('happiness.n.02.happiness')]\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "print(wn.synsets(\"happy\")[0])\n",
    "print(wn.synsets(\"happy\")[0].lemmas()[0].antonyms())\n",
    "print(wn.synsets(\"happy\")[0].lemmas()[0].derivationally_related_forms())\n",
    "print(wn.synsets(\"happy\")[0].lemmas()[0].count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the basic similarity measures mentioned in class (and several others) are available in the NLTK WordNet interface, as are other functions which are used in their derivation. For similarity metrics which require information content, we can load statistics from available corpora (the SEMCOR and Brown corpora are popular options)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     /Users/laujh/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet_ic.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n",
      "0.875\n",
      "0.5763952661933001\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet_ic\n",
    "import nltk\n",
    "nltk.download('wordnet_ic')\n",
    "\n",
    "print(wn.synset(\"book.n.02\").path_similarity(wn.synset(\"newspaper.n.03\")))\n",
    "print(wn.synset(\"book.n.02\").wup_similarity(wn.synset(\"newspaper.n.03\")))\n",
    "\n",
    "semcor_ic = wordnet_ic.ic('ic-semcor.dat')\n",
    "\n",
    "print(wn.synset(\"book.n.02\").lin_similarity(wn.synset(\"newspaper.n.03\"),semcor_ic))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, they are somewhat opaque in their operation, and only work on synsets. Let's create a version of basic path distance which doesn't require you to select a specific synset in advance, and shows you the exact path through the WordNet heirarchy that the score is based on. There are many ways to do this, we'll do it in a fairly clear but not entirely optimal way. First, given a set of synsets, let's get a dictionary where the keys correspond to all hypernym synsets, and the values are the next step below on the shortest past to one of the initial synsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Synset('publication.n.01'): Synset('book.n.01'), Synset('product.n.02'): Synset('book.n.02'), Synset('fact.n.02'): Synset('record.n.05'), Synset('dramatic_composition.n.01'): Synset('script.n.01'), Synset('record.n.07'): Synset('ledger.n.01'), Synset('collection.n.01'): Synset('book.n.06'), Synset('sacred_text.n.01'): Synset('bible.n.01'), Synset('section.n.01'): Synset('book.n.10'), Synset('writing.n.02'): Synset('sacred_text.n.01'), Synset('creation.n.02'): Synset('product.n.02'), Synset('group.n.01'): Synset('collection.n.01'), Synset('music.n.01'): Synset('section.n.01'), Synset('work.n.02'): Synset('publication.n.01'), Synset('document.n.03'): Synset('record.n.07'), Synset('information.n.01'): Synset('fact.n.02'), Synset('artifact.n.01'): Synset('creation.n.02'), Synset('auditory_communication.n.01'): Synset('music.n.01'), Synset('communication.n.02'): Synset('document.n.03'), Synset('abstraction.n.06'): Synset('group.n.01'), Synset('written_communication.n.01'): Synset('writing.n.02'), Synset('message.n.02'): Synset('information.n.01'), Synset('entity.n.01'): Synset('abstraction.n.06'), Synset('whole.n.02'): Synset('artifact.n.01'), Synset('object.n.01'): Synset('whole.n.02'), Synset('physical_entity.n.01'): Synset('object.n.01')}\n"
     ]
    }
   ],
   "source": [
    "def get_hypernym_path_dict(synsets):\n",
    "    hypernym_dict = {}\n",
    "    synsets_to_expand = synsets\n",
    "    while synsets_to_expand:\n",
    "        new_synsets_to_expand = set()\n",
    "        for synset in synsets_to_expand:\n",
    "            for hypernym in synset.hypernyms():\n",
    "                if hypernym not in hypernym_dict:  # this ensures we get the shortest path\n",
    "                    hypernym_dict[hypernym] = synset\n",
    "                    new_synsets_to_expand.add(hypernym)\n",
    "        synsets_to_expand = new_synsets_to_expand\n",
    "    return hypernym_dict\n",
    "        \n",
    "        \n",
    "hypernym_dict = get_hypernym_path_dict(wn.synsets(\"book\",\"n\"))\n",
    "print(hypernym_dict)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a way to build the path using this information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('physical_entity.n.01'), Synset('object.n.01'), Synset('whole.n.02'), Synset('artifact.n.01'), Synset('creation.n.02'), Synset('product.n.02'), Synset('book.n.02')]\n"
     ]
    }
   ],
   "source": [
    "def get_path_using_hypernym_dict(hypernym,hypernym_dict,synsets):\n",
    "    path = [hypernym]\n",
    "    current_synset = hypernym_dict[hypernym]\n",
    "    while current_synset not in synsets:\n",
    "        path.append(current_synset)\n",
    "        current_synset =  hypernym_dict[current_synset]\n",
    "    path.append(current_synset)\n",
    "    return path\n",
    "    \n",
    "print(get_path_using_hypernym_dict(wn.synset('physical_entity.n.01'),hypernym_dict,wn.synsets(\"book\",\"n\")))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we can build ancestor dictionaries for each of the words, look at the intersection, and then find the shortest path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n",
      "[Synset('book.n.02'), Synset('product.n.02'), Synset('newspaper.n.03')]\n",
      "0.2\n",
      "[Synset('dog.n.03'), Synset('chap.n.01'), Synset('male.n.02'), Synset('man.n.01'), Synset('guy.n.01')]\n",
      "['guy', 'cat', 'hombre', 'bozo']\n",
      "0.2\n",
      "[Synset('nickel.n.02'), Synset('coin.n.01'), Synset('coinage.n.01'), Synset('currency.n.01'), Synset('money.n.03')]\n",
      "0.09090909090909091\n",
      "[Synset('calculator.n.01'), Synset('expert.n.01'), Synset('person.n.01'), Synset('causal_agent.n.01'), Synset('physical_entity.n.01'), Synset('matter.n.03'), Synset('substance.n.07'), Synset('food.n.01'), Synset('nutriment.n.01'), Synset('dish.n.02'), Synset('pizza.n.01')]\n",
      "1.0\n",
      "[Synset('movie.n.01')]\n"
     ]
    }
   ],
   "source": [
    "def get_shortest_path_between(word1,word2):\n",
    "    synsets1 = wn.synsets(word1)\n",
    "    synsets2 = wn.synsets(word2)\n",
    "    # added these two lines to catch situation where word1 and word2 share a synset\n",
    "    match = set(synsets1).intersection(set(synsets2))\n",
    "    if match: return [list(match)[0]]\n",
    "\n",
    "    hypernym_dict1 = get_hypernym_path_dict(synsets1)\n",
    "    hypernym_dict2 = get_hypernym_path_dict(synsets2)\n",
    "    best_path = []\n",
    "    for hypernym in hypernym_dict1:\n",
    "        if hypernym in hypernym_dict2 and hypernym_dict1[hypernym] != hypernym_dict2[hypernym]:\n",
    "            path1 = get_path_using_hypernym_dict(hypernym,hypernym_dict1,synsets1)\n",
    "            path2 = get_path_using_hypernym_dict(hypernym,hypernym_dict2,synsets2)\n",
    "            if not best_path or len(path1) + len(path2) - 1 < len(best_path):\n",
    "                path1.reverse()\n",
    "                best_path = path1 + path2[1:]\n",
    "    return best_path\n",
    "\n",
    "path = get_shortest_path_between(\"book\",\"newspaper\")\n",
    "print(1.0/len(path))\n",
    "print(path)\n",
    "\n",
    "path = get_shortest_path_between(\"dog\",\"cat\")\n",
    "print(1.0/len(path))\n",
    "print(path)\n",
    "# to see that the last synset includes the \"cat\" lemma\n",
    "print(path[-1].lemma_names())\n",
    "\n",
    "path = get_shortest_path_between(\"nickel\",\"money\")\n",
    "print(1.0/len(path))\n",
    "print(path)\n",
    "\n",
    "path = get_shortest_path_between(\"computer\",\"pizza\")\n",
    "print(1.0/len(path))\n",
    "print(path)\n",
    "\n",
    "path = get_shortest_path_between(\"film\",\"movie\")\n",
    "print(1.0/len(path))\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shortest path does not always correspond to the most obvious relationship between two words: for instance, newspaper and book are join as products (not reading materials), dog and cat by informal senses related to people, rather than animals. Using depth and information-content basic metrics can improve this situation. Another approach is to use the counts of lemmas to ignore rare senses. Note that doing all this for other metrics is somewhat different, because they are based on the idea of lowest common subsumer, which is not necessarily on the shortest path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
