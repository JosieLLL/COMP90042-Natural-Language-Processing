{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11-machine-translation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOM9DPlAGyzg",
        "colab_type": "text"
      },
      "source": [
        "# Neural Machine Translation\n",
        "\n",
        "In this workshop, we are going to build a seq2seq machine translation model and train it on a parallel corpus of English and French. We will frame the translation problem in a slightly different way. Instead of translating the sentence word by word, we are going to work on **character-level**. This means, tokens in the source and target sentences are characters instead of words.\n",
        "\n",
        "We'll be using the [Keras](https://keras.io/) framework. This notebook adopted code in this [blog post](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html).\n",
        "\n",
        "It might take hours to train this model on CPUs, hence we encorage you to run this experiment with GPU support on [Colab](https://https://colab.research.google.com/). After you have uploaded this notebook to Colab, don't forget to enable GPU acceleration by going to \"Runtime  >  Change runtime type\" and selecting \"GPU\" as the hardware accelerator. Click save.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwrmaJ5bGfnq",
        "colab_type": "code",
        "outputId": "ed50b340-750e-4c55-9e16-5c4cf0a6993e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "!pip install keras"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.3.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.4)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgmU9rhTWzrB",
        "colab_type": "text"
      },
      "source": [
        "Before we start, let's download the data set and unzip it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaL-QiLvKCRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import urllib.request\n",
        "from zipfile import ZipFile\n",
        "\n",
        "if not os.path.exists('fra-eng.zip'):\n",
        "  url = 'http://www.manythings.org/anki/fra-eng.zip'\n",
        "\n",
        "  opener = urllib.request.URLopener()\n",
        "  opener.addheader('User-Agent', ' ')\n",
        "  filename, headers = opener.retrieve(url, 'fra-eng.zip')\n",
        "\n",
        "  compressed = ZipFile(filename, \"r\")\n",
        "  compressed.extractall()\n",
        "  compressed.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DSOmkX1Xp7Z",
        "colab_type": "text"
      },
      "source": [
        "Each row in the file is a pair of sentences in English and French, repectively. The two sentences are separated by `\\t`. The first step if our pre-processing is to read pairs of sentences in the data file. Meanwhile, we will also build the character vocabulary of the input and output languages.\n",
        "\n",
        "Note that the goal of machine translation is to generate sequences in the target language. Therefore, for target sequences, we will need ***start of sequence*** and ***end of sequence*** symbol to denote the start and end of generation, respectively. You will see why this is essential in the inference process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFTeV8SWgC2j",
        "colab_type": "code",
        "outputId": "5e6fce9d-1545-43ef-e920-df686a7e7ca8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "data_path = 'fra.txt'\n",
        "num_samples = 10000\n",
        "\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text, _ = line.split('\\t')\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    target_text = '\\t' + target_text + '\\n'\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    # build input character vocab\n",
        "    for char in input_text:\n",
        "      if char not in input_characters:\n",
        "          input_characters.add(char)\n",
        "    # build target character vocab\n",
        "    for char in target_text:\n",
        "      if char not in target_characters:\n",
        "          target_characters.add(char)\n",
        "\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "print('Number of samples:', len(input_texts))\n",
        "print('Number of unique input tokens:', num_encoder_tokens)\n",
        "print('Number of unique output tokens:', num_decoder_tokens)\n",
        "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
        "print('Max sequence length for outputs:', max_decoder_seq_length)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples: 10000\n",
            "Number of unique input tokens: 71\n",
            "Number of unique output tokens: 93\n",
            "Max sequence length for inputs: 16\n",
            "Max sequence length for outputs: 59\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SyWhZpoc-4r",
        "colab_type": "code",
        "outputId": "7b887641-74ee-4363-df89-39f8645cda80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "input_texts[1], target_texts[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Hi.', '\\tSalut !\\n')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Yj1NON-srkE",
        "colab_type": "text"
      },
      "source": [
        "After we have the data in textual format, we need to convert them into vectors that can be fed into our model. Turn the sentences into 3 Numpy arrays, encoder_input_data, decoder_input_data, decoder_target_data:\n",
        "\n",
        "1. `encoder_input_data` is a 3D array of shape (num_pairs, max_english_sentence_length, num_english_characters) containing a one-hot vectorization of the English sentences.\n",
        "\n",
        "2. `decoder_input_data` is a 3D array of shape (num_pairs, max_french_sentence_length, num_french_characters) containg a one-hot vectorization of the French sentences.\n",
        "\n",
        "3. `decoder_target_data` is the same as decoder_input_data but offset by one timestep. decoder_target_data[:, t, :] will be the same as decoder_input_data[:, t + 1, :]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2OYuu4WlNmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# build character to index lookup for source language\n",
        "input_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(input_characters)])\n",
        "# build character to index lookup for target language\n",
        "target_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(target_characters)])\n",
        "\n",
        "# initilize the 3D arrays with zeros\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "\n",
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
        "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1. # add paddings from t+1 to max_encoder_seq_length\n",
        "    for t, char in enumerate(target_text):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character because we are not interested in generating it.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
        "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1. # add paddings from t+1 to max_decoder_seq_length\n",
        "    decoder_target_data[i, t:, target_token_index[' ']] = 1. # add paddings from t to max_encoder_seq_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcROp5O9we2l",
        "colab_type": "text"
      },
      "source": [
        "# What is sequence-to-sequence learning?\n",
        "Sequence-to-sequence learning (Seq2Seq) is about training models to convert sequences from one domain (e.g. sentences in English) to sequences in another domain (e.g. the same sentences translated to French).\n",
        "\n",
        "```\n",
        "\"the cat sat on the mat\" -> [Seq2Seq model] -> \"le chat etait assis sur le tapis\"\n",
        "```\n",
        "This can be used for machine translation or for free-from question answering (generating a natural language answer given a natural language question) -- in general, it is applicable any time you need to generate text.\n",
        "\n",
        "There are multiple ways to handle this task, either using RNNs or using 1D convnets. Here we will focus on RNNs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxoXWibHs-QT",
        "colab_type": "text"
      },
      "source": [
        "# The trivial case: when input and output sequences have the same length\n",
        "When both input sequences and output sequences have the same length, you can implement such models simply with a Keras LSTM or GRU layer (or stack thereof). This is the case in this example script that shows how to teach a RNN to learn to add numbers, encoded as character strings:\n",
        "\n",
        "![](https://blog.keras.io/img/seq2seq/addition-rnn.png)\n",
        "\n",
        "One caveat of this approach is that it assumes that it is possible to generate target[...t] given input[...t]. That works in some cases (e.g. adding strings of digits) but does not work for most use cases. In the general case, information about the entire input sequence is necessary in order to start generating the target sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EINt4hB4f3sc",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# The general case: canonical sequence-to-sequence\n",
        "In the general case, input sequences and output sequences have different lengths (e.g. machine translation) and the entire input sequence is required in order to start predicting the target. This requires a more advanced setup, which is what people commonly refer to when mentioning \"sequence to sequence models\" with no further context. Here's how it works:\n",
        "\n",
        "* A RNN layer (or stack thereof) acts as \"encoder\": it processes the input \n",
        "sequence and returns its own internal state. Note that we discard the outputs of the encoder RNN, only recovering the state. This state will serve as the \"context\", or \"conditioning\", of the decoder in the next step.\n",
        "\n",
        "* Another RNN layer (or stack thereof) acts as \"decoder\": it is trained to predict the next characters of the target sequence, given previous characters of the target sequence. Specifically, it is trained to turn the target sequences into the same sequences but offset by one timestep in the future, a training process called \"teacher forcing\" in this context. Importantly, the encoder uses as initial state the state vectors from the encoder, which is how the decoder obtains information about what it is supposed to generate. Effectively, the decoder learns to generate targets[t+1...] given targets[...t], conditioned on the input sequence.\n",
        "\n",
        "![](https://blog.keras.io/img/seq2seq/seq2seq-teacher-forcing.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCWVDrXP3eEz",
        "colab_type": "text"
      },
      "source": [
        "This is our training model. It leverages three key features of Keras RNNs:\n",
        "\n",
        "* The `return_state` contructor argument, configuring a RNN layer to return a list where the first entry is the outputs and the next entries are the internal RNN states. This is used to recover the states of the encoder.\n",
        "\n",
        "* The `inital_state` call argument, specifying the initial state(s) of a RNN. This is used to pass the encoder states to the decoder as initial states.\n",
        "The `return_sequences` constructor argument, configuring a RNN to return its full sequence of outputs (instead of just the last output, which the defaults behavior). This is used in the decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd7wKaEhlO0t",
        "colab_type": "code",
        "outputId": "64a5a9bb-fb71-4644-8706-547c86af9503",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        }
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "\n",
        "latent_dim = 256 # size of the states\n",
        "\n",
        "# Define an input sequence and process it.\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
        "                                     initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, None, 71)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, None, 93)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 256), (None, 335872      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, None, 256),  358400      input_2[0][0]                    \n",
            "                                                                 lstm_1[0][1]                     \n",
            "                                                                 lstm_1[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, None, 93)     23901       lstm_2[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 718,173\n",
            "Trainable params: 718,173\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5pczlK_w0sp",
        "colab_type": "text"
      },
      "source": [
        "You might have noticed that the decoder LSTM has slightly more number of trainable parameters than the encoder. This is because the size of target vocab is slightly bigger than the size of input vocab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIupD5hv28BD",
        "colab_type": "text"
      },
      "source": [
        "We train our model in two lines, while monitoring the loss on a held-out set of 20% of the samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5I7NqlD47ah",
        "colab_type": "code",
        "outputId": "d6632a14-dc29-4cb9-e63e-da4bd2844f81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "batch_size = 256  # Batch size for training.\n",
        "epochs = 30  # Number of epochs to train for\n",
        "\n",
        "# Run training\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_split=0.2)\n",
        "# Save model\n",
        "model.save('s2s.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8000 samples, validate on 2000 samples\n",
            "Epoch 1/100\n",
            "8000/8000 [==============================] - 18s 2ms/step - loss: 1.1716 - accuracy: 0.7257 - val_loss: 1.0481 - val_accuracy: 0.7091\n",
            "Epoch 2/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.8537 - accuracy: 0.7698 - val_loss: 0.8523 - val_accuracy: 0.7635\n",
            "Epoch 3/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.6759 - accuracy: 0.8087 - val_loss: 0.7188 - val_accuracy: 0.7911\n",
            "Epoch 4/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.5909 - accuracy: 0.8286 - val_loss: 0.6473 - val_accuracy: 0.8109\n",
            "Epoch 5/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.5428 - accuracy: 0.8415 - val_loss: 0.6010 - val_accuracy: 0.8239\n",
            "Epoch 6/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.5064 - accuracy: 0.8512 - val_loss: 0.5741 - val_accuracy: 0.8307\n",
            "Epoch 7/100\n",
            "8000/8000 [==============================] - 16s 2ms/step - loss: 0.4777 - accuracy: 0.8589 - val_loss: 0.5497 - val_accuracy: 0.8380\n",
            "Epoch 8/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.4542 - accuracy: 0.8653 - val_loss: 0.5281 - val_accuracy: 0.8430\n",
            "Epoch 9/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.4325 - accuracy: 0.8715 - val_loss: 0.5126 - val_accuracy: 0.8486\n",
            "Epoch 10/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.4139 - accuracy: 0.8764 - val_loss: 0.4977 - val_accuracy: 0.8521\n",
            "Epoch 11/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.3970 - accuracy: 0.8813 - val_loss: 0.4878 - val_accuracy: 0.8550\n",
            "Epoch 12/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.3817 - accuracy: 0.8855 - val_loss: 0.4777 - val_accuracy: 0.8583\n",
            "Epoch 13/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.3668 - accuracy: 0.8901 - val_loss: 0.4754 - val_accuracy: 0.8587\n",
            "Epoch 14/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.3530 - accuracy: 0.8942 - val_loss: 0.4658 - val_accuracy: 0.8627\n",
            "Epoch 15/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.3403 - accuracy: 0.8980 - val_loss: 0.4549 - val_accuracy: 0.8652\n",
            "Epoch 16/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.3281 - accuracy: 0.9014 - val_loss: 0.4507 - val_accuracy: 0.8677\n",
            "Epoch 17/100\n",
            "8000/8000 [==============================] - 16s 2ms/step - loss: 0.3166 - accuracy: 0.9047 - val_loss: 0.4481 - val_accuracy: 0.8675\n",
            "Epoch 18/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.3052 - accuracy: 0.9083 - val_loss: 0.4460 - val_accuracy: 0.8692\n",
            "Epoch 19/100\n",
            "8000/8000 [==============================] - 16s 2ms/step - loss: 0.2951 - accuracy: 0.9111 - val_loss: 0.4413 - val_accuracy: 0.8706\n",
            "Epoch 20/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.2850 - accuracy: 0.9141 - val_loss: 0.4395 - val_accuracy: 0.8712\n",
            "Epoch 21/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.2756 - accuracy: 0.9166 - val_loss: 0.4356 - val_accuracy: 0.8734\n",
            "Epoch 22/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.2661 - accuracy: 0.9194 - val_loss: 0.4386 - val_accuracy: 0.8727\n",
            "Epoch 23/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.2577 - accuracy: 0.9220 - val_loss: 0.4336 - val_accuracy: 0.8751\n",
            "Epoch 24/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.2491 - accuracy: 0.9243 - val_loss: 0.4387 - val_accuracy: 0.8745\n",
            "Epoch 25/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.2412 - accuracy: 0.9267 - val_loss: 0.4393 - val_accuracy: 0.8752\n",
            "Epoch 26/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.2337 - accuracy: 0.9289 - val_loss: 0.4384 - val_accuracy: 0.8750\n",
            "Epoch 27/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.2261 - accuracy: 0.9311 - val_loss: 0.4419 - val_accuracy: 0.8757\n",
            "Epoch 28/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.2192 - accuracy: 0.9330 - val_loss: 0.4452 - val_accuracy: 0.8756\n",
            "Epoch 29/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.2124 - accuracy: 0.9351 - val_loss: 0.4529 - val_accuracy: 0.8741\n",
            "Epoch 30/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.2057 - accuracy: 0.9372 - val_loss: 0.4496 - val_accuracy: 0.8763\n",
            "Epoch 31/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1998 - accuracy: 0.9388 - val_loss: 0.4499 - val_accuracy: 0.8762\n",
            "Epoch 32/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1938 - accuracy: 0.9408 - val_loss: 0.4566 - val_accuracy: 0.8752\n",
            "Epoch 33/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1880 - accuracy: 0.9426 - val_loss: 0.4640 - val_accuracy: 0.8752\n",
            "Epoch 34/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1830 - accuracy: 0.9439 - val_loss: 0.4600 - val_accuracy: 0.8766\n",
            "Epoch 35/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1775 - accuracy: 0.9452 - val_loss: 0.4666 - val_accuracy: 0.8758\n",
            "Epoch 36/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1726 - accuracy: 0.9470 - val_loss: 0.4727 - val_accuracy: 0.8754\n",
            "Epoch 37/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1677 - accuracy: 0.9485 - val_loss: 0.4753 - val_accuracy: 0.8750\n",
            "Epoch 38/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1632 - accuracy: 0.9499 - val_loss: 0.4811 - val_accuracy: 0.8745\n",
            "Epoch 39/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1583 - accuracy: 0.9513 - val_loss: 0.4845 - val_accuracy: 0.8747\n",
            "Epoch 40/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1545 - accuracy: 0.9525 - val_loss: 0.4852 - val_accuracy: 0.8762\n",
            "Epoch 41/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1501 - accuracy: 0.9538 - val_loss: 0.5007 - val_accuracy: 0.8742\n",
            "Epoch 42/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1465 - accuracy: 0.9547 - val_loss: 0.4926 - val_accuracy: 0.8757\n",
            "Epoch 43/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1423 - accuracy: 0.9562 - val_loss: 0.5048 - val_accuracy: 0.8741\n",
            "Epoch 44/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1388 - accuracy: 0.9572 - val_loss: 0.5061 - val_accuracy: 0.8746\n",
            "Epoch 45/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1357 - accuracy: 0.9579 - val_loss: 0.5090 - val_accuracy: 0.8744\n",
            "Epoch 46/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1320 - accuracy: 0.9592 - val_loss: 0.5167 - val_accuracy: 0.8748\n",
            "Epoch 47/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1290 - accuracy: 0.9602 - val_loss: 0.5238 - val_accuracy: 0.8742\n",
            "Epoch 48/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1261 - accuracy: 0.9607 - val_loss: 0.5181 - val_accuracy: 0.8754\n",
            "Epoch 49/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1227 - accuracy: 0.9621 - val_loss: 0.5287 - val_accuracy: 0.8743\n",
            "Epoch 50/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1200 - accuracy: 0.9627 - val_loss: 0.5276 - val_accuracy: 0.8750\n",
            "Epoch 51/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1172 - accuracy: 0.9637 - val_loss: 0.5398 - val_accuracy: 0.8745\n",
            "Epoch 52/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1148 - accuracy: 0.9642 - val_loss: 0.5392 - val_accuracy: 0.8743\n",
            "Epoch 53/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1123 - accuracy: 0.9652 - val_loss: 0.5452 - val_accuracy: 0.8744\n",
            "Epoch 54/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1094 - accuracy: 0.9656 - val_loss: 0.5500 - val_accuracy: 0.8743\n",
            "Epoch 55/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1074 - accuracy: 0.9663 - val_loss: 0.5506 - val_accuracy: 0.8749\n",
            "Epoch 56/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1049 - accuracy: 0.9672 - val_loss: 0.5584 - val_accuracy: 0.8747\n",
            "Epoch 57/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1024 - accuracy: 0.9678 - val_loss: 0.5594 - val_accuracy: 0.8758\n",
            "Epoch 58/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.1009 - accuracy: 0.9683 - val_loss: 0.5653 - val_accuracy: 0.8744\n",
            "Epoch 59/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0985 - accuracy: 0.9688 - val_loss: 0.5698 - val_accuracy: 0.8746\n",
            "Epoch 60/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0965 - accuracy: 0.9692 - val_loss: 0.5738 - val_accuracy: 0.8738\n",
            "Epoch 61/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0947 - accuracy: 0.9697 - val_loss: 0.5764 - val_accuracy: 0.8741\n",
            "Epoch 62/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0924 - accuracy: 0.9706 - val_loss: 0.5836 - val_accuracy: 0.8737\n",
            "Epoch 63/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0906 - accuracy: 0.9710 - val_loss: 0.5902 - val_accuracy: 0.8737\n",
            "Epoch 64/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0886 - accuracy: 0.9719 - val_loss: 0.5980 - val_accuracy: 0.8737\n",
            "Epoch 65/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0873 - accuracy: 0.9719 - val_loss: 0.5914 - val_accuracy: 0.8744\n",
            "Epoch 66/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0854 - accuracy: 0.9727 - val_loss: 0.5971 - val_accuracy: 0.8741\n",
            "Epoch 67/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0839 - accuracy: 0.9731 - val_loss: 0.6065 - val_accuracy: 0.8735\n",
            "Epoch 68/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0819 - accuracy: 0.9736 - val_loss: 0.6055 - val_accuracy: 0.8745\n",
            "Epoch 69/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0809 - accuracy: 0.9738 - val_loss: 0.6128 - val_accuracy: 0.8735\n",
            "Epoch 70/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0791 - accuracy: 0.9744 - val_loss: 0.6207 - val_accuracy: 0.8731\n",
            "Epoch 71/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0778 - accuracy: 0.9748 - val_loss: 0.6178 - val_accuracy: 0.8727\n",
            "Epoch 72/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0764 - accuracy: 0.9751 - val_loss: 0.6239 - val_accuracy: 0.8739\n",
            "Epoch 73/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0751 - accuracy: 0.9756 - val_loss: 0.6333 - val_accuracy: 0.8732\n",
            "Epoch 74/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0738 - accuracy: 0.9758 - val_loss: 0.6299 - val_accuracy: 0.8735\n",
            "Epoch 75/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0716 - accuracy: 0.9766 - val_loss: 0.6372 - val_accuracy: 0.8729\n",
            "Epoch 76/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0713 - accuracy: 0.9767 - val_loss: 0.6352 - val_accuracy: 0.8735\n",
            "Epoch 77/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0700 - accuracy: 0.9770 - val_loss: 0.6380 - val_accuracy: 0.8734\n",
            "Epoch 78/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0685 - accuracy: 0.9774 - val_loss: 0.6423 - val_accuracy: 0.8742\n",
            "Epoch 79/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0674 - accuracy: 0.9778 - val_loss: 0.6478 - val_accuracy: 0.8729\n",
            "Epoch 80/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0664 - accuracy: 0.9782 - val_loss: 0.6558 - val_accuracy: 0.8732\n",
            "Epoch 81/100\n",
            "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0648 - accuracy: 0.9783 - val_loss: 0.6589 - val_accuracy: 0.8725\n",
            "Epoch 82/100\n",
            "8000/8000 [==============================] - 16s 2ms/step - loss: 0.0640 - accuracy: 0.9788 - val_loss: 0.6578 - val_accuracy: 0.8738\n",
            "Epoch 83/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0628 - accuracy: 0.9791 - val_loss: 0.6686 - val_accuracy: 0.8722\n",
            "Epoch 84/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0625 - accuracy: 0.9792 - val_loss: 0.6667 - val_accuracy: 0.8728\n",
            "Epoch 85/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0609 - accuracy: 0.9796 - val_loss: 0.6714 - val_accuracy: 0.8729\n",
            "Epoch 86/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0601 - accuracy: 0.9797 - val_loss: 0.6713 - val_accuracy: 0.8733\n",
            "Epoch 87/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0594 - accuracy: 0.9799 - val_loss: 0.6762 - val_accuracy: 0.8732\n",
            "Epoch 88/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0584 - accuracy: 0.9803 - val_loss: 0.6857 - val_accuracy: 0.8714\n",
            "Epoch 89/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0577 - accuracy: 0.9804 - val_loss: 0.6799 - val_accuracy: 0.8721\n",
            "Epoch 90/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0566 - accuracy: 0.9807 - val_loss: 0.6841 - val_accuracy: 0.8722\n",
            "Epoch 91/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0558 - accuracy: 0.9810 - val_loss: 0.6920 - val_accuracy: 0.8722\n",
            "Epoch 92/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0548 - accuracy: 0.9812 - val_loss: 0.6924 - val_accuracy: 0.8725\n",
            "Epoch 93/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0534 - accuracy: 0.9816 - val_loss: 0.6951 - val_accuracy: 0.8727\n",
            "Epoch 94/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0533 - accuracy: 0.9817 - val_loss: 0.7003 - val_accuracy: 0.8717\n",
            "Epoch 95/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0524 - accuracy: 0.9819 - val_loss: 0.7076 - val_accuracy: 0.8718\n",
            "Epoch 96/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0521 - accuracy: 0.9820 - val_loss: 0.7021 - val_accuracy: 0.8716\n",
            "Epoch 97/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0509 - accuracy: 0.9825 - val_loss: 0.7034 - val_accuracy: 0.8726\n",
            "Epoch 98/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0504 - accuracy: 0.9824 - val_loss: 0.7126 - val_accuracy: 0.8720\n",
            "Epoch 99/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0498 - accuracy: 0.9828 - val_loss: 0.7098 - val_accuracy: 0.8728\n",
            "Epoch 100/100\n",
            "8000/8000 [==============================] - 15s 2ms/step - loss: 0.0490 - accuracy: 0.9830 - val_loss: 0.7176 - val_accuracy: 0.8722\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Rwqc9BJqspS",
        "colab_type": "text"
      },
      "source": [
        "# Inference in encoder-decoder architecture\n",
        "![](https://blog.keras.io/img/seq2seq/seq2seq-inference.png)\n",
        "\n",
        "In inference mode, i.e. when we want to decode unknown input sequences, we go through a slightly different process:\n",
        "\n",
        "1. Encode the input sequence into state vectors.\n",
        "2. Start with a target sequence of size 1 (just the start-of-sequence character).\n",
        "3. Feed the state vectors and 1-char target sequence to the decoder to produce predictions for the next character.\n",
        "4. Sample the next character using these predictions (we simply use argmax).\n",
        "5. Append the sampled character to the target sequence\n",
        "6. Repeat until we generate the end-of-sequence character or we hit the character limit.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_Kx-uTN4KBb",
        "colab_type": "text"
      },
      "source": [
        "We define our inference model in this cell. Here's the drill:\n",
        "\n",
        "1. Encode input and retrieve initial decoder state\n",
        "2. Run one step of decoder with this initial state and a ***start of sequence*** token as target. Output will be the next target token\n",
        "3. Repeat with the current target token and current states\n",
        "\n",
        "The decoder needs to run iteratively while the encoder only need to run once. This means, the encoder model can stay the same while we slightly tweak the decoder model. So we define encoder and decoder separately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvBdPSbH4_Ci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define sampling models\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim,)) # decoder hidden state\n",
        "decoder_state_input_c = Input(shape=(latent_dim,)) # decoder cell state\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "# Decoder takes characters from 'decoder_inputs' and states in 'decoder_state_inputs' as inputs.\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c] # we wrap hidden and cell states by a list\n",
        "decoder_outputs = decoder_dense(decoder_outputs) # project decoder output to logits and then apply softmax\n",
        "\n",
        "# Define the model that will turn\n",
        "# `decoder_inputs` & `decoder__states_inputs` into `decoder_outputs` & `decoder_states`\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + decoder_states_inputs,\n",
        "    [decoder_outputs] + decoder_states)\n",
        "\n",
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict(\n",
        "    (i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict(\n",
        "    (i, char) for char, i in target_token_index.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fzj-FKp3Feq",
        "colab_type": "text"
      },
      "source": [
        "To decode a test sentence, we will repeatedly:\n",
        "\n",
        "1. Encode the input sentence and retrieve the initial decoder state\n",
        "2. Run one step of the decoder with this initial state and a ***start of sequence*** token as target. The output will be the next target character.\n",
        "3. Append the target character predicted and repeat."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWf1wWic5IyP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index] # map token index back to character\n",
        "        decoded_sentence += sampled_char # concatenate the results with the newly generated character\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_char == '\\n' or\n",
        "           len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ij1EzxtYrXUq",
        "colab_type": "text"
      },
      "source": [
        "Now we use some examples from the training set to test our inference model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykruR-Qy5KKK",
        "colab_type": "code",
        "outputId": "e9bf186d-a295-463e-b7b7-c01da6411a7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for seq_index in range(100):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', input_texts[seq_index])\n",
        "    print('Decoded sentence:', decoded_sentence)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: Go.\n",
            "Decoded sentence: Va !\n",
            "\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Salut.\n",
            "\n",
            "-\n",
            "Input sentence: Hi.\n",
            "Decoded sentence: Salut.\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Courez !\n",
            "\n",
            "-\n",
            "Input sentence: Run!\n",
            "Decoded sentence: Courez !\n",
            "\n",
            "-\n",
            "Input sentence: Who?\n",
            "Decoded sentence: Qui ?\n",
            "\n",
            "-\n",
            "Input sentence: Wow!\n",
            "Decoded sentence: Ça alors !\n",
            "\n",
            "-\n",
            "Input sentence: Fire!\n",
            "Decoded sentence: Au feu !\n",
            "\n",
            "-\n",
            "Input sentence: Help!\n",
            "Decoded sentence: À l'aide !\n",
            "\n",
            "-\n",
            "Input sentence: Jump.\n",
            "Decoded sentence: Saute.\n",
            "\n",
            "-\n",
            "Input sentence: Stop!\n",
            "Decoded sentence: Stop !\n",
            "\n",
            "-\n",
            "Input sentence: Stop!\n",
            "Decoded sentence: Stop !\n",
            "\n",
            "-\n",
            "Input sentence: Stop!\n",
            "Decoded sentence: Stop !\n",
            "\n",
            "-\n",
            "Input sentence: Wait!\n",
            "Decoded sentence: Attendez !\n",
            "\n",
            "-\n",
            "Input sentence: Wait!\n",
            "Decoded sentence: Attendez !\n",
            "\n",
            "-\n",
            "Input sentence: Go on.\n",
            "Decoded sentence: Poursuis.\n",
            "\n",
            "-\n",
            "Input sentence: Go on.\n",
            "Decoded sentence: Poursuis.\n",
            "\n",
            "-\n",
            "Input sentence: Go on.\n",
            "Decoded sentence: Poursuis.\n",
            "\n",
            "-\n",
            "Input sentence: Hello!\n",
            "Decoded sentence: Salut !\n",
            "\n",
            "-\n",
            "Input sentence: Hello!\n",
            "Decoded sentence: Salut !\n",
            "\n",
            "-\n",
            "Input sentence: I see.\n",
            "Decoded sentence: Je vois une lispotie.\n",
            "\n",
            "-\n",
            "Input sentence: I try.\n",
            "Decoded sentence: J'essaye.\n",
            "\n",
            "-\n",
            "Input sentence: I won!\n",
            "Decoded sentence: Je l'ai emporté !\n",
            "\n",
            "-\n",
            "Input sentence: I won!\n",
            "Decoded sentence: Je l'ai emporté !\n",
            "\n",
            "-\n",
            "Input sentence: I won.\n",
            "Decoded sentence: J’ai gagné.\n",
            "\n",
            "-\n",
            "Input sentence: Oh no!\n",
            "Decoded sentence: Oh non !\n",
            "\n",
            "-\n",
            "Input sentence: Attack!\n",
            "Decoded sentence: Attaque !\n",
            "\n",
            "-\n",
            "Input sentence: Attack!\n",
            "Decoded sentence: Attaque !\n",
            "\n",
            "-\n",
            "Input sentence: Cheers!\n",
            "Decoded sentence: Santé !\n",
            "\n",
            "-\n",
            "Input sentence: Cheers!\n",
            "Decoded sentence: Santé !\n",
            "\n",
            "-\n",
            "Input sentence: Cheers!\n",
            "Decoded sentence: Santé !\n",
            "\n",
            "-\n",
            "Input sentence: Cheers!\n",
            "Decoded sentence: Santé !\n",
            "\n",
            "-\n",
            "Input sentence: Get up.\n",
            "Decoded sentence: Lève-toi.\n",
            "\n",
            "-\n",
            "Input sentence: Go now.\n",
            "Decoded sentence: Va, maintenant.\n",
            "\n",
            "-\n",
            "Input sentence: Go now.\n",
            "Decoded sentence: Va, maintenant.\n",
            "\n",
            "-\n",
            "Input sentence: Go now.\n",
            "Decoded sentence: Va, maintenant.\n",
            "\n",
            "-\n",
            "Input sentence: Got it!\n",
            "Decoded sentence: Compris !\n",
            "\n",
            "-\n",
            "Input sentence: Got it!\n",
            "Decoded sentence: Compris !\n",
            "\n",
            "-\n",
            "Input sentence: Got it?\n",
            "Decoded sentence: Compris ?\n",
            "\n",
            "-\n",
            "Input sentence: Got it?\n",
            "Decoded sentence: Compris ?\n",
            "\n",
            "-\n",
            "Input sentence: Got it?\n",
            "Decoded sentence: Compris ?\n",
            "\n",
            "-\n",
            "Input sentence: Hop in.\n",
            "Decoded sentence: Montez.\n",
            "\n",
            "-\n",
            "Input sentence: Hop in.\n",
            "Decoded sentence: Montez.\n",
            "\n",
            "-\n",
            "Input sentence: Hug me.\n",
            "Decoded sentence: Serrez-moi dans vos bras !\n",
            "\n",
            "-\n",
            "Input sentence: Hug me.\n",
            "Decoded sentence: Serrez-moi dans vos bras !\n",
            "\n",
            "-\n",
            "Input sentence: I fell.\n",
            "Decoded sentence: Je suis tombé.\n",
            "\n",
            "-\n",
            "Input sentence: I fell.\n",
            "Decoded sentence: Je suis tombé.\n",
            "\n",
            "-\n",
            "Input sentence: I know.\n",
            "Decoded sentence: Je sais.\n",
            "\n",
            "-\n",
            "Input sentence: I left.\n",
            "Decoded sentence: Je suis parti.\n",
            "\n",
            "-\n",
            "Input sentence: I left.\n",
            "Decoded sentence: Je suis parti.\n",
            "\n",
            "-\n",
            "Input sentence: I lied.\n",
            "Decoded sentence: J'ai menti.\n",
            "\n",
            "-\n",
            "Input sentence: I lost.\n",
            "Decoded sentence: J'ai perdu.\n",
            "\n",
            "-\n",
            "Input sentence: I paid.\n",
            "Decoded sentence: J’ai payé.\n",
            "\n",
            "-\n",
            "Input sentence: I'm 19.\n",
            "Decoded sentence: J'ai trinté.\n",
            "\n",
            "-\n",
            "Input sentence: I'm OK.\n",
            "Decoded sentence: Je suis en train de discuter.\n",
            "\n",
            "-\n",
            "Input sentence: I'm OK.\n",
            "Decoded sentence: Je suis en train de discuter.\n",
            "\n",
            "-\n",
            "Input sentence: Listen.\n",
            "Decoded sentence: Écoutez !\n",
            "\n",
            "-\n",
            "Input sentence: No way!\n",
            "Decoded sentence: C'est exclu !\n",
            "\n",
            "-\n",
            "Input sentence: No way!\n",
            "Decoded sentence: C'est exclu !\n",
            "\n",
            "-\n",
            "Input sentence: No way!\n",
            "Decoded sentence: C'est exclu !\n",
            "\n",
            "-\n",
            "Input sentence: No way!\n",
            "Decoded sentence: C'est exclu !\n",
            "\n",
            "-\n",
            "Input sentence: No way!\n",
            "Decoded sentence: C'est exclu !\n",
            "\n",
            "-\n",
            "Input sentence: No way!\n",
            "Decoded sentence: C'est exclu !\n",
            "\n",
            "-\n",
            "Input sentence: No way!\n",
            "Decoded sentence: C'est exclu !\n",
            "\n",
            "-\n",
            "Input sentence: No way!\n",
            "Decoded sentence: C'est exclu !\n",
            "\n",
            "-\n",
            "Input sentence: No way!\n",
            "Decoded sentence: C'est exclu !\n",
            "\n",
            "-\n",
            "Input sentence: Really?\n",
            "Decoded sentence: Vraiment ?\n",
            "\n",
            "-\n",
            "Input sentence: Really?\n",
            "Decoded sentence: Vraiment ?\n",
            "\n",
            "-\n",
            "Input sentence: Really?\n",
            "Decoded sentence: Vraiment ?\n",
            "\n",
            "-\n",
            "Input sentence: Thanks.\n",
            "Decoded sentence: Merci !\n",
            "\n",
            "-\n",
            "Input sentence: We try.\n",
            "Decoded sentence: On essaye.\n",
            "\n",
            "-\n",
            "Input sentence: We won.\n",
            "Decoded sentence: Nous avons gagné.\n",
            "\n",
            "-\n",
            "Input sentence: We won.\n",
            "Decoded sentence: Nous avons gagné.\n",
            "\n",
            "-\n",
            "Input sentence: We won.\n",
            "Decoded sentence: Nous avons gagné.\n",
            "\n",
            "-\n",
            "Input sentence: We won.\n",
            "Decoded sentence: Nous avons gagné.\n",
            "\n",
            "-\n",
            "Input sentence: Ask Tom.\n",
            "Decoded sentence: Demande à Tom.\n",
            "\n",
            "-\n",
            "Input sentence: Awesome!\n",
            "Decoded sentence: Fantastique !\n",
            "\n",
            "-\n",
            "Input sentence: Be calm.\n",
            "Decoded sentence: Soyez calme !\n",
            "\n",
            "-\n",
            "Input sentence: Be calm.\n",
            "Decoded sentence: Soyez calme !\n",
            "\n",
            "-\n",
            "Input sentence: Be calm.\n",
            "Decoded sentence: Soyez calme !\n",
            "\n",
            "-\n",
            "Input sentence: Be cool.\n",
            "Decoded sentence: Sois détendu !\n",
            "\n",
            "-\n",
            "Input sentence: Be fair.\n",
            "Decoded sentence: Soyez équitable !\n",
            "\n",
            "-\n",
            "Input sentence: Be fair.\n",
            "Decoded sentence: Soyez équitable !\n",
            "\n",
            "-\n",
            "Input sentence: Be fair.\n",
            "Decoded sentence: Soyez équitable !\n",
            "\n",
            "-\n",
            "Input sentence: Be fair.\n",
            "Decoded sentence: Soyez équitable !\n",
            "\n",
            "-\n",
            "Input sentence: Be fair.\n",
            "Decoded sentence: Soyez équitable !\n",
            "\n",
            "-\n",
            "Input sentence: Be fair.\n",
            "Decoded sentence: Soyez équitable !\n",
            "\n",
            "-\n",
            "Input sentence: Be kind.\n",
            "Decoded sentence: Sois gentil.\n",
            "\n",
            "-\n",
            "Input sentence: Be nice.\n",
            "Decoded sentence: Soyez gentille !\n",
            "\n",
            "-\n",
            "Input sentence: Be nice.\n",
            "Decoded sentence: Soyez gentille !\n",
            "\n",
            "-\n",
            "Input sentence: Be nice.\n",
            "Decoded sentence: Soyez gentille !\n",
            "\n",
            "-\n",
            "Input sentence: Be nice.\n",
            "Decoded sentence: Soyez gentille !\n",
            "\n",
            "-\n",
            "Input sentence: Be nice.\n",
            "Decoded sentence: Soyez gentille !\n",
            "\n",
            "-\n",
            "Input sentence: Be nice.\n",
            "Decoded sentence: Soyez gentille !\n",
            "\n",
            "-\n",
            "Input sentence: Beat it.\n",
            "Decoded sentence: Dégage !\n",
            "\n",
            "-\n",
            "Input sentence: Call me.\n",
            "Decoded sentence: Appelle-moi !\n",
            "\n",
            "-\n",
            "Input sentence: Call me.\n",
            "Decoded sentence: Appelle-moi !\n",
            "\n",
            "-\n",
            "Input sentence: Call us.\n",
            "Decoded sentence: Appelez-nous !\n",
            "\n",
            "-\n",
            "Input sentence: Call us.\n",
            "Decoded sentence: Appelez-nous !\n",
            "\n",
            "-\n",
            "Input sentence: Come in.\n",
            "Decoded sentence: Entrez !\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}