{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train BPE on a toy text example\n",
    "\n",
    "bpe algorithm: https://web.stanford.edu/~jurafsky/slp3/2.pdf (2.4.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab = defaultdict(<class 'int'>, {'T h e </w>': 2, 'a i m s </w>': 1, 'f o r </w>': 4, 't h i s </w>': 1, 's u b j e c t </w>': 1, 'i s </w>': 2, 's t u d e n t s </w>': 1, 't o </w>': 2, 'd e v e l o p </w>': 1, 'a n </w>': 1, 'u n d e r s t a n d i n g </w>': 1, 'o f </w>': 2, 't h e </w>': 2, 'm a i n </w>': 1, 'a l g o r i t h m s </w>': 1, 'u s e d </w>': 2, 'i n </w>': 3, 'n a t u r a l l a n g u a g e </w>': 1, 'p r o c e s s i n g , </w>': 1, 'u s e </w>': 2, 'a </w>': 1, 'd i v e r s e </w>': 1, 'r a n g e </w>': 1, 'a p p l i c a t i o n s </w>': 1, 'i n c l u d i n g </w>': 1, 't e x t </w>': 1, 'c l a s s i f i c a t i o n , </w>': 1, 'm a c h i n e </w>': 1, 't r a n s l a t i o n , </w>': 1, 'a n d </w>': 3, 'q u e s t i o n </w>': 1, 'a n s w e r i n g . </w>': 1, 'T o p i c s </w>': 1, 'b e </w>': 1, 'c o v e r e d </w>': 1, 'i n c l u d e </w>': 1, 'p a r t - o f - s p e e c h </w>': 1, 't a g g i n g , </w>': 1, 'n - g r a m </w>': 1, 'l a n g u a g e </w>': 2, 'm o d e l l i n g , </w>': 1, 's y n t a c t i c </w>': 1, 'p a r s i n g </w>': 1, 'd e e p </w>': 1, 'l e a r n i n g . </w>': 1, 'p r o g r a m m i n g </w>': 1, 'P y t h o n , </w>': 1, 's e e </w>': 1, 'm o r e </w>': 1, 'i n f o r m a t i o n </w>': 1, 'o n </w>': 1, 'i t s </w>': 1, 'w o r k s h o p s , </w>': 1, 'a s s i g n m e n t s </w>': 1, 'i n s t a l l a t i o n </w>': 1, 'a t </w>': 1, 'h o m e . </w>': 1})\n",
      "==========\n",
      "Tokens Before BPE\n",
      "Tokens: defaultdict(<class 'int'>, {'T': 3, 'h': 11, 'e': 39, '</w>': 72, 'a': 38, 'i': 37, 'm': 12, 's': 34, 'f': 9, 'o': 29, 'r': 22, 't': 29, 'u': 14, 'b': 2, 'j': 1, 'c': 13, 'd': 15, 'n': 45, 'v': 3, 'l': 16, 'p': 11, 'g': 22, ',': 7, 'x': 1, 'q': 1, 'w': 2, '.': 3, '-': 3, 'y': 2, 'P': 1, 'k': 1})\n",
      "Number of tokens: 31\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "import re, collections\n",
    "\n",
    "text = \"The aims for this subject is for students to develop an understanding of the main algorithms used in naturallanguage processing, for use in a diverse range of applications including text classification, machine translation, and question answering. Topics to be covered include part-of-speech tagging, n-gram language modelling, syntactic parsing and deep learning. The programming language used is Python, see for more information on its use in the workshops, assignments and installation at home.\"\n",
    "# text = 'low '*5 +'lower '*2+'newest '*6 +'widest '*3\n",
    "def get_vocab(text):\n",
    "    vocab = collections.defaultdict(int)\n",
    "    for word in text.strip().split():\n",
    "        #note: we use the special token </w> (instead of underscore in the lecture) to denote the end of a word\n",
    "        vocab[' '.join(list(word)) + ' </w>'] += 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "This function iterates through all words in the vocabulary and count pair of tokens which are next to each other.\n",
    "\n",
    "EXAMPLE:\n",
    "    word = 'T h e <\\w>'\n",
    "    pairs of tokens in this word [('T', 'h'), ('h', 'e'), ('e', '<\\w>')]\n",
    "    \n",
    "INPUT:\n",
    "    vocab: Dict[str, int]  # The vocabulary, a counter for word frequency\n",
    "    \n",
    "OUTPUT:\n",
    "    pairs: Dict[Tuple[str, str], int] # Word pairs, a counter for pair frequency\n",
    "\n",
    "\"\"\"\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    ###\n",
    "    # Your answer BEGINS HERE\n",
    "    ###\n",
    "    for word, freq in vacab.items():                 #{'T h e <\\w>' : int}\n",
    "        letters = word.split()                       #['T','h','e','<\\w>']\n",
    "        for i in range(len(letters) - 1):            #index 0,1,2,3\n",
    "            pairs[letters[i], letters[i+1]] += freq  #{('T','h'): int}\n",
    "    ###\n",
    "    # Your answer ENDS HERE\n",
    "    ###\n",
    "    return pairs\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This function merges a given pair of tokens in all words in the vocabulary\n",
    "\n",
    "EXAMPLE:\n",
    "    word = 'T h e <\\w>'\n",
    "    pair = ('e', '<\\w>')\n",
    "    word_after_merge = 'T h e<\\w>'\n",
    "    \n",
    "Input:\n",
    "    pair: Tuple[str, str] # the pair of tokens need to be merged\n",
    "    v_in: Dict[str, int]  # vocabulary before merge\n",
    "    \n",
    "Output:\n",
    "    v_out: Dict[str, int] # vocabulary after merge\n",
    "    \n",
    "HINT:\n",
    "    When merging pair ('h', 'e') for word 'Th e<\\w>', the two tokens in this word 'Th' and 'e<\\w>' shouldn't be merged.\n",
    "\n",
    "\"\"\"\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    ###\n",
    "    # Your answer BEGINS HERE\n",
    "    ###\n",
    "    \n",
    "    \n",
    "    ###\n",
    "    # Your answer ENDS HERE\n",
    "    ###\n",
    "    return v_out\n",
    "\n",
    "def get_tokens(vocab):\n",
    "    tokens = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        word_tokens = word.split()\n",
    "        for token in word_tokens:\n",
    "            tokens[token] += freq\n",
    "    return tokens\n",
    "\n",
    "\n",
    "vocab = get_vocab(text)\n",
    "print(\"Vocab =\", vocab)\n",
    "print('==========')\n",
    "print('Tokens Before BPE')\n",
    "tokens = get_tokens(vocab)\n",
    "print('Tokens: {}'.format(tokens))\n",
    "print('Number of tokens: {}'.format(len(tokens)))\n",
    "print('==========')\n",
    "\n",
    "#about 100 merges we start to see common words\n",
    "num_merges = 100\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    if not pairs:\n",
    "        break\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    new_token = ''.join(best)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "    print('Iter: {}'.format(i))\n",
    "    print('Best pair: {}'.format(best))\n",
    "    # add new token to the vocab\n",
    "    tokens[new_token] = pairs[best]\n",
    "    # deduct frequency for tokens have been merged\n",
    "    tokens[best[0]] -= pairs[best]\n",
    "    tokens[best[1]] -= pairs[best]\n",
    "    print('Tokens: {}'.format(tokens))\n",
    "    print('Number of tokens: {}'.format(len(tokens)))\n",
    "    print('==========')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After training, used the BPE dictionaries to tokenise sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tokens_from_vocab(vocab):\n",
    "    tokens_frequencies = collections.defaultdict(int)\n",
    "    vocab_tokenization = {}\n",
    "    for word, freq in vocab.items():\n",
    "        word_tokens = word.split()\n",
    "        for token in word_tokens:\n",
    "            tokens_frequencies[token] += freq\n",
    "        vocab_tokenization[''.join(word_tokens)] = word_tokens\n",
    "    return tokens_frequencies, vocab_tokenization\n",
    "\n",
    "def measure_token_length(token):\n",
    "    if token[-4:] == '</w>':\n",
    "        return len(token[:-4]) + 1\n",
    "    else:\n",
    "        return len(token)\n",
    "    \n",
    "def tokenize_word(string, sorted_tokens, unknown_token='</u>'):\n",
    "    \n",
    "    if string == '':\n",
    "        return []\n",
    "    if sorted_tokens == []:\n",
    "        return [unknown_token] * len(string)\n",
    "\n",
    "    string_tokens = []\n",
    "    # iterate over all tokens to find match\n",
    "    for i in range(len(sorted_tokens)):\n",
    "        token = sorted_tokens[i]\n",
    "        token_reg = re.escape(token.replace('.', '[.]'))\n",
    "        matched_positions = [(m.start(0), m.end(0)) for m in re.finditer(token_reg, string)]\n",
    "        # if no match found in the string, go to next token\n",
    "        if len(matched_positions) == 0:\n",
    "            continue\n",
    "        # collect end position of matches in the string\n",
    "        substring_end_positions = [matched_position[0] for matched_position in matched_positions]\n",
    "        substring_start_position = 0\n",
    "        for substring_end_position in substring_end_positions:\n",
    "            # slice for sub-word\n",
    "            substring = string[substring_start_position:substring_end_position]\n",
    "            # tokenize this sub-word with tokens remaining\n",
    "            string_tokens += tokenize_word(string=substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)\n",
    "            string_tokens += [token]\n",
    "            substring_start_position = substring_end_position + len(token)\n",
    "        # tokenize the remaining string\n",
    "        remaining_substring = string[substring_start_position:]\n",
    "        string_tokens += tokenize_word(string=remaining_substring, sorted_tokens=sorted_tokens[i+1:], unknown_token=unknown_token)\n",
    "        break\n",
    "    else:\n",
    "        # return list of unknown token if no match is found for the string\n",
    "        string_tokens = [unknown_token] * len(string)\n",
    "        \n",
    "    return string_tokens\n",
    "\n",
    "\"\"\"\n",
    "This function generates a list of all tokens sorted by their length (1st key) and frequency (2nd key).\n",
    "\n",
    "EXAMPLE:\n",
    "    token frequency dictionary before sorting: {'natural': 3, 'language':2, 'processing': 4, 'lecture': 4}\n",
    "    sorted tokens: ['processing', 'language', 'lecture', 'natural']\n",
    "    \n",
    "INPUT:\n",
    "    token_frequencies: Dict[str, int] # Counter for token frequency\n",
    "    \n",
    "OUTPUT:\n",
    "    sorted_token: List[str] # Tokens sorted by length and frequency\n",
    "\n",
    "\"\"\"\n",
    "def sort_tokens(tokens_frequencies):\n",
    "    ###\n",
    "    # Your answer BEGINS HERE\n",
    "    ###\n",
    "    \n",
    "    \n",
    "    ###\n",
    "    # Your answer ENDS HERE\n",
    "    ###\n",
    "    return sorted_tokens\n",
    "\n",
    "#display the vocab\n",
    "tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
    "\n",
    "#sort tokens by length and frequency\n",
    "sorted_tokens = sort_tokens(tokens_frequencies)\n",
    "print(\"Tokens =\", sorted_tokens, \"\\n\")\n",
    "\n",
    "sentence_1 = 'I like natural language processing!'\n",
    "sentence_2 = 'I like natural languaaage processing!'\n",
    "sentence_list = [sentence_1, sentence_2]\n",
    "\n",
    "for sentence in sentence_list:\n",
    "    \n",
    "    print('==========')\n",
    "    print(\"Sentence =\", sentence)\n",
    "    \n",
    "    for word in sentence.split():\n",
    "        word = word + \"</w>\"\n",
    "\n",
    "        print('Tokenizing word: {}...'.format(word))\n",
    "        if word in vocab_tokenization:\n",
    "            print(vocab_tokenization[word])\n",
    "        else:\n",
    "            print(tokenize_word(string=word, sorted_tokens=sorted_tokens, unknown_token='</u>'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
